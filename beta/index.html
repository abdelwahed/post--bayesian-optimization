<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Exploring Bayesian Optimization</title>
  <script defer src="js/template.v2.js"></script>
  <link rel="stylesheet" type="text/css" href="css/styles.css">
  <script src="https://d3js.org/d3.v4.min.js"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/d3-legend/2.25.6/d3-legend.js"></script>
  <script defer src="js/plot.js"></script>
  <script defer src="js/hider.js"></script>
  <script defer src="js/slides.js"></script>
  <script type="text/javascript" src="https://d3js.org/d3-scale-chromatic.v1.min.js"></script>

</head>

<body>

  <d-front-matter>
    <script id='distill-front-matter' type="text/json">
      {
        "title": "Exploring Bayesian Optimization",
        "description": "Breaking Bayesian Optimization into small, sizeable chunks.",
        "authors": [{
            "author": "Apoorv Agnihotri",
            "authorURL": "https://apoorvagnihotri.github.io/",
            "affiliations": [{
              "name": "Indian Insitute of Technology Gandhinagar",
              "affiliationURL": "https://www.iitgn.ac.in/"
            }]
          },
          {
            "author": "Nipun Batra",
            "authorURL": "https://nipunbatra.github.io/",
            "affiliations": [{
              "name": "Indian Insitute of Technology Gandhinagar",
              "affiliationURL": "https://www.iitgn.ac.in/"
            }]
          }
        ],
        "katex": {
          "delimiters": [{
            "left": "$$",
            "right": "$$",
            "display": false
          }]
        }
      }
    </script>
  </d-front-matter>

  <d-title style="padding-bottom: 0">
    <p>Breaking Bayesian Optimization into small, sizeable chunks.</p>
    <div class="l-screen shaded-figure" style="border-bottom: none">
      <div class="slideshow-container">

        <div class="mySlides fade">
          <div class="numbertext">1 / 13</div>
          <img src="images/slides/Slide1.jpg" style="width:100%">
        </div>
        
        <div class="mySlides fade">
          <div class="numbertext">2 / 13</div>
          <img src="images/slides/Slide2.jpg" style="width:100%">
        </div>
        
        <div class="mySlides fade">
          <div class="numbertext">3 / 13</div>
          <img src="images/slides/Slide3.jpg" style="width:100%">
        </div>
        
        <div class="mySlides fade">
          <div class="numbertext">4 / 13</div>
          <img src="images/slides/Slide4.jpg" style="width:100%">
        </div>
                
        <div class="mySlides fade">
          <div class="numbertext">5 / 13</div>
          <img src="images/slides/Slide5.jpg" style="width:100%">
        </div>
              
        <div class="mySlides fade">
          <div class="numbertext">6 / 13</div>
          <img src="images/slides/Slide6.jpg" style="width:100%">
        </div>
      
        <div class="mySlides fade">
          <div class="numbertext">7 / 13</div>
          <img src="images/slides/Slide7.jpg" style="width:100%">
        </div>

        <div class="mySlides fade">
          <div class="numbertext">8 / 13</div>
          <img src="images/slides/Slide8.jpg" style="width:100%">
        </div>

        <div class="mySlides fade">
          <div class="numbertext">9 / 13</div>
          <img src="images/slides/Slide9.jpg" style="width:100%">
        </div>

        <div class="mySlides fade">
          <div class="numbertext">10 / 13</div>
          <img src="images/slides/Slide10.jpg" style="width:100%">
        </div>

        <div class="mySlides fade">
          <div class="numbertext">11 / 13</div>
          <img src="images/slides/Slide11.jpg" style="width:100%">
        </div>

        <div class="mySlides fade">
          <div class="numbertext">12 / 13</div>
          <img src="images/slides/Slide12.jpg" style="width:100%">
        </div>

        <div class="mySlides fade">
          <div class="numbertext">13 / 13</div>
          <img src="images/slides/Slide13.jpg" style="width:100%">
        </div>
        
        <a class="prev" onclick="plusSlides(-1)">&#10094;</a>
        <a class="next" onclick="plusSlides(1)">&#10095;</a>
        
        </div>
        <br>
        
        <div style="text-align:center">
          <span class="dot" onclick="currentSlide(1)"></span> 
          <span class="dot" onclick="currentSlide(2)"></span> 
          <span class="dot" onclick="currentSlide(3)"></span> 
          <span class="dot" onclick="currentSlide(4)"></span> 
          <span class="dot" onclick="currentSlide(5)"></span> 
          <span class="dot" onclick="currentSlide(6)"></span> 
          <span class="dot" onclick="currentSlide(7)"></span> 
          <span class="dot" onclick="currentSlide(8)"></span> 
          <span class="dot" onclick="currentSlide(9)"></span> 
          <span class="dot" onclick="currentSlide(10)"></span> 
          <span class="dot" onclick="currentSlide(11)"></span> 
          <span class="dot" onclick="currentSlide(12)"></span> 
          <span class="dot" onclick="currentSlide(13)"></span> 
      </div>
    </div>
  </d-title>

  <d-byline></d-byline>

  <d-article style="overflow-x: unset;">
    <p>
      We are increasingly getting used to machine learning algorithms with a large number of hyperparameters. For example, neural networks with hyperparameters like; the number of layers, the dropout rate, the learning rate, among others. Or random forests with hyperparameters such as the number of trees, and the maximum depth. How do we efficiently tune these hyperparameters to optimize our machine learning model? In this article, we will talk about Bayesian Optimization (BO) - which is an effective suite of techniques often used to efficiently tune the hyperparameters. Besides being used in tuning hyperparameters, BO is a general suite of techniques for optimizing any black-box function.
    </p>

    <p>
      Before we talk in depth about Bayesian Optimization and its applicability in hyperparameter tuning, we will look into maximizing (optimizing) a black box function!
    </p>

    <h1>Mining Gold!</h1>
    <p>
      Let us start the discussion with the example of mining for gold. Our goal is to mine for gold in a new, unknown land. For now, let us make a simplifying assumption, the gold content lies in a one-dimensional space, i.e., we are talking gold distribution only about a line. Our aim is to find the location along this line where we would get the maximum return and drill at that location. Now, at the start of the activity, we have no idea about the amount of gold at different locations. The only way we can get the information about the amount of gold is by drilling at different locations. This drilling is costly and involves expensive sensors to be used. We, therefore, want to <strong>minimize the number of drillings required</strong> while still being able to <strong>find the location of maximum gold quickly</strong>.
    </p>

    <p>
      Shown below are two common objectives for the gold mining problem.
    </p>

    <ul>
      <li>
        <p>
          <strong>Problem 1: Best Estimate of Gold Distribution</strong><br/>
          In this problem, we want to estimate the amount of gold on the one-dimensional line. Since the cost of drillings limits us, we can not drill at every location. We should drill at those locations that provide us with the <strong>maximum information</strong> about the distribution of the gold. This problem is akin to 
          <strong>
            active learning<d-cite key="settles2009active,Tong2001"></d-cite>
          </strong>.
        </p>
      </li>

      <li>
        <p>
          <strong>Problem 2: Location of Maximum Gold</strong><br/>
          In this problem, we want to find the location in the one-dimensional space where the gold quantity is the maximum. This problem focuses on finding the location with the most gold content. This problem is akin to 
          <strong>
            Bayesian Optimization<d-cite key="humanOut,nandoBOtut"></d-cite>
          </strong>.
        </p>
      </li>
    </ul>

    <p>
      In this article, we will soon see how the two problems are related, 
      but not the same.
    </p>



    <h2>Active Learning</h2>

    <p>
      In machine learning problems, often, unlabelled data is very easily available, but labeling could be an expensive task. As an example, for a speech-to-text task, the ground truth labeling or annotation task requires expert(s) to label words and sentences manually. In our gold mining problem, drilling (akin to labeling) is an expensive operation, and we would like to minimize it. But, without drilling, we can not estimate the gold estimate. Active learning would solve this problem by posing a smart strategy to choose the next drilling site. While there are various methods and techniques in the active learning literature, for the sake of brevity, we will look only at <strong>uncertainty reduction</strong>. This method chooses the next query point as the one our model is the most uncertain about. One of the ways we can reduce the uncertainty is by choosing the point at which we have the maximum variance (we are most uncertain).
    </p>

    <h3>Surrogate Model</h3>
    <p>
      Active learning, along with BO, which we will soon see later, rely on a model for modeling the unknown ground truth function <d-math>f(x)</d-math>. This model is called the surrogate model. The surrogate model should be chosen such that it is able to model the ground truth function closely. Here <d-math>f(x)</d-math> denotes the actual gold content on our new land, and unquestionably we don't know <d-math>f(x)</d-math>.
    </p>

    <h3>Bayesian Update</h3>
    <p>
      The updatation of the surrogate model is the "Bayes" in BO. Every evaluation (drilling to get the gold estimate) of <d-math>f(x)</d-math> gives the surrogate model more data to learn from. The posterior for the surrogate is obtained using the Bayes Rule with this new data at every iteration. At the end of an iteration the posterior becomes the prior, for the next cycle.
    </p>

    <p>
      It is common in the literature to use Gaussian Processes as a surrogate. The priors of a GP can be set by using specific kernels and mean functions. Further, GPs give us not only the predictions but also the uncertainty estimates, which we will find to be useful in active learning, as well as BO.
    </p>

    <h3>Gaussian Processes (GP)</h3>

    <p>
      One might want to look at this excellent distillpub article<d-cite key="görtler2019a"></d-cite> on Gaussian Processes<d-cite key="Rasmussen2004"></d-cite>.
      We will be using GP regression to model the gold distribution along the one-dimensional line. By using GP, we assume that the gold distribution of nearby points is similar (smoothness). Such an assumption is usually valid. 
    </p>

    <p>
      Let us now try to see how our ground truth function <d-math>f(x)</d-math> looks like. The gold distribution in our data looks to be bi-modal, with a maximum value around <d-math>x = 5</d-math>. For now, let us not worry about the X-axis or the
      Y-axis units.
    </p>
    <figure class="smaller-img">
      <d-figure><img src="images/MAB_gifs/GT.svg" /></d-figure>
    </figure>


    <h4 id="priormodel">Prior Model</h4>

    <p>
      We will choose a simple prior to model the gold content along a one-dimensional space. Our prior assumes a smooth relationship between points via a Matern kernel
      <d-footnote>
        See <a href="http://www.gaussianprocess.org/gpml/">Rasmussen and Williams 2004</a> and <a href="https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Matern.html">scikit-learn</a>, for details regarding the Matern kernel.
      </d-footnote>.
      The black line in the graph below denotes the knowledge we have about the gold content without drilling even at a single location.
    </p>

    <figure>
      <d-figure><img src="images/MAB_gifs/prior.svg" /></d-figure>
    </figure>

    <p>
      Also, take notice that the confidence (uncertainty) about the gold content is also the same for every location.
    </p>

    <h4 id="addingtrainingdata">Adding Training Data</h4>

    <p>
      Let us now add a point to the train set or in other words, drill one of the locations and see the gold content (<d-math>y</d-math>). We can see how our confidence and our estimates change after we get this first       information by fitting the model to the new data. We are going to add <d-math>(x = 0.5, \ y = f(0.5))</d-math> into the train set now.
    </p>

    <figure>
      <d-figure><img src="images/MAB_gifs/posterior.svg" /></d-figure>
    </figure>

    <p>
      We see now that the GP posterior (shown as our prediction) has changed and we are very certain about the gold content in the vicinity of <d-math>x = 0.5</d-math>, but, very uncertain far away from it. Also, we can see that
      the mean of the point
      closer to <d-math>x = 0.5</d-math> is closer to the value that we got from drilling and seeing the gold content<d-footnote>You might notice that there might be cases when we would actually predict the gold content to be negative. This issue is not central to understanding BO here, but we can still overcome this by making our GP to regress over <d-math>\log\left(f(x)\right)</d-math> instead of <d-math>f(x)</d-math>.</d-footnote>. So, we now come to the key idea of active learning:
    </p>

    <h3 id="activelearningprocedure">Active Learning Procedure
    </h3>

    <ol>
      <li>Choose the point of having the highest uncertainty</li>
      <li>Add the point to train set</li>
      <li>Train on the new train set</li>
      <li>Go to #1 till convergence or budget elapsed</li>
    </ol>

    <p>
      Let us now simulate this process and see how our posterior changes at every iteration (after each drilling). For each of our iteration below, the prior was the Gaussian Process learned on the points already in the training set.
    </p>
    <figure>
      <d-figure><img src="images/MAB_gifs/active-gp.gif" /></d-figure>
    </figure>

    <p>
      One point to notice is that this idea of choosing the most uncertain location leads to querying of the points that are the farthest (visible when we choose the
      2nd location to drill). Through this animation, we can notice that just in a few iterations, we are able to estimate the true distribution of gold. At every iteration, our active learning procedure carries out <strong> exploration
      </strong> to make our estimates better.
    </p>

    <h2 id="bayesianoptimization">Bayesian Optimization</h2>
    <p>
      <strong>Problem 2</strong> requires us to find the location where the gold content is maximum. Even though the problem setting may be similar, the objective is quite different than Problem 1. The present problem deals with finding the location where our black-box function reaches maximum. In contrast, the earlier problem focuses on getting a good estimate of the black-box function.
    </p>
<!-- 
    <p>
      Older problem - Earlier in the active learning problem, our motivation for drilling at locations was to predict the distribution of the gold content over all the locations in the one-dimensional line. We, therefore, had chosen the next
      location to drill where we had maximum uncertainty about our estimate.
    </p> -->

    <p>
      Given the fact that we are only interested in knowing the location where the maximum occurs, it might be a good idea to evaluate at locations where our surrogate model's predicted mean is the highest, i.e. to <strong>exploit</strong>. But unfortunately, our model mean is not always accurate (since we have limited observations), so we need to correct our model, which can be done by reducing variance or <strong>exploration</strong>. BO looks at both <strong>exploitation</strong> and <strong>exploration</strong>, whereas in the case of active learning, we only cared about <strong>exploration</strong>.
    </p>

    <h1>Formalizing Bayesian Optimization</h1>

      <span style="padding-bottom: 1em;">Let us now formally introduce Bayesian Optimization. Our goal is to find the <d-math>{x}</d-math> where we reached global maximum (or minimum) of a function <d-math>f: \mathbb{R}^d \mapsto \mathbb{R}</d-math>. Constraints in
        Bayesian Optimization look like below quoted from slides/talk
        <d-footnote>Talk from Peter Fraizer from Uber on Bayesian Optimization:<br>
          <ul>
            <li> <a href="https://www.youtube.com/watch?v=c4KKvyWW_Xk">Youtube Talk</a></li>
            <li> <a href="https://people.orie.cornell.edu/pfrazier/Presentations/2018.11.INFORMS.tutorial.pdf">Slide Deck</a></li>
          </ul>
        </d-footnote> on Tutorial on Bayesian Optimization from Peter Fraizer.</span>


      <p>We’d like to optimize <d-math>f: \mathbb{R}^d \mapsto \mathbb{R}</d-math>.<br></p>

      <ul>
        <li>
          <d-math>f</d-math>’s feasible set <d-math>A</d-math> is simple,
          e.g., box constraints.
        </li>
        <li>
          <d-math>f</d-math> is continuous but lacks special structure,
          e.g., concavity, that would make it easy to optimize.
        </li>
        <li>
          <d-math>f</d-math> is derivative-free:
          evaluations do not give gradient information.
        </li>
        <li>
          <d-math>f</d-math> is expensive to evaluate:
          the number of times we can evaluate it
          is severely limited.
        </li>
        <li>
          <d-math>f</d-math> may be noisy. If noise is present, we’ll assume it
          is independent and normally distributed, with
          common but unknown variance.
        </li>
      </ul>


      <p>Let us link the above constraints to our initial problem statement of gold mining.</p>

      <ul>
        <li>Our domain in the gold mining problem is a single-dimensional box constraint of <d-math>0 \leq x \leq 6</d-math>.</li>

        <li>Our ground truth can be seen as neither convex nor concave function, which resulted in local minima as well.</li>

        <li>Our evaluation (by drilling) of the amount of gold content at a location didn't give us any gradient information.</li>

        <li>The function we used in the case of gold mining problem is extremely costly to evaluate (drilling costs millions).</li>

        <li>This constraint is still satisfied in our case as we had have taken noiseless measurements. Which can be considered 
          as a Gaussian noise with zero mean and zero standard deviation.</li>
      </ul>

      <p>
        We see that our original problem of mining gold fits into the criterions required to use BO. We will now introduce the additional topics that we require before we can get that maximal gold.
      </p>

    <h3>Acquisition Functions</h3>

    <p>
      We just discussed that our original optimization problem <d-math>x^* = \text{argmax}_{x \in A} f(x)</d-math> is hard given the expensive nature of evaluating <d-math>f</d-math>. The key idea of BO is to transform this original difficult optimization into a sequence of easier inexpensive optimizations of functions called an acquisition function (<d-math>\alpha(x)</d-math>). Each of these sequence of easier inexpensive optimizations involves finding the next point to sample. Thus, we can interpret the acquisition function as commensurate with how desirable evaluating <d-math>f</d-math> at <d-math>x</d-math> is expected to be for the maximisation problem<d-footnote>Please find <a href="https://www.cse.wustl.edu/~garnett/cse515t/spring_2015/files/lecture_notes/12.pdf">these</a> slides from Washington University in St. Louis to know more.</d-footnote>.
    </p>
    <p>
      While we have just now discussed that our goal is to transform the original optimisation into a sequence of easier optimisation, where is the "Bayesian" in this optimisation, and how is the acquisition function related? Let us re-wind and go back to our surrogate model and build the link between all the things we have discussed thus far, by noting the steps of BO<d-footnote>Please find <a href="https://youtu.be/EnXxO3BAgYk">this</a> amazing video from Javier González at The Gaussian Process Summer School 2019.</d-footnote>
    </p>
    <p>
      <ol>
        <li>
          Choose a surrogate model and its prior over space of objectives <d-math>f</d-math>.
        </li>
        <li>
          Given the set of observations (function sampling), use Bayes rule to obtain the posterior.
        </li>
        <li>
          Use an acquisition function <d-math>\alpha(x)</d-math>, which is a function of the posterior to decide where to sample next <d-math>x_t = \text{argmax}_x \alpha(x)</d-math>.
        </li>
        <li>
          Add new sampled data to the set of observations and Goto Step #2 till convergence or budget elapses.
        </li>
      </ol>
    </p>
    <p>
      We now have three core ideas associated with acquisition functions: i) they are a function of the surrogate posterior; ii) they combine exploration and exploitation; and iii) they are inexpensive to evaluate. Let us now look into a few examples of commonly used acquisition functions to understand the concept better.
    </p>
<!-- 
    <p>
      Now, to take into account the combination of exploration and exploitation, we try to use a function that combines the two aspects. These utility functions are called acquisition functions. These functions can be considered a function of the posterior of the surrogate model we might be using.
    </p>

    <p>
      We can write a general form of an acquisition function (<d-math>\alpha(x)</d-math>) as a function of the mean(<d-math>\mu(x)</d-math>) and the variance(<d-math>\sigma(x)</d-math>), which is in turn specifying that <d-math>\alpha(x)
      </d-math> is a function of exploration and exploitation.
      <d-math block>
        \alpha(x) = g(\mu(x), \sigma(x))
      </d-math>
      At each iteration <d-math>t</d-math>, we would recompute
      <d-math>\alpha(x) </d-math> and choose the location
      <d-math> x_t = _\text{argmax}\alpha_t(x)</d-math>
      as the next location/point to query.
    </p> -->

    <h3>Probability of Improvement (PI)</h3>

    <p>
      Let us start with the most common acquisition function in the BO literature. The idea behind the algorithm is fairly simple - choose the next point as the one which has the highest probability of improvement over the current max <d-math>f(x^+)</d-math>, where <d-math> x^+ = \text{argmax}_{x_i \in x_{1:t}}f(x_i)</d-math> and <d-math>x_i</d-math> is the location queried at <d-math>i^{th}</d-math> time step. We pointed out earlier that the acquisition functions are a function of the posterior. Therefore, we can look at PI as one such function. Looking closely, we see PI is essentially the left tail probability of the posterior.
    </p>

    <!-- 
      <p>Let us understand this concept in two cases:</p>

      <ol>
        <li>
          <p>
            We have two points of similar means (of function values (gold in our case)). We now want to choose one of these to obtain the labels or values. We will choose the one with higher variance. This basically says that <strong>given the same exploitability, we choose the one with higher exploration value</strong>.
          </p>
        </li>
        <li>
          <p>
            We have two points having the same variance. We would now choose the point with the higher mean. This basically says that <strong>given the same explorability, we will choose the one with higher exploitation value</strong>.
          </p>
        </li>
        </ol> 
      -->
    
    <p>The formulation or procedure can be given as:</p>

    <ol>
      <li>
        <p>
          Let <d-math>f(x^+)</d-math> be the current highest value of the function
        </p>
      </li>
      <li>
        <p>
          Let <d-math>\epsilon</d-math> be close to zero
        </p>
      </li>

      <li>
        <p>
          Choose <d-math>x_{t+1} = argmax(\alpha_{PI}(x))</d-math> where <d-math>\alpha_{PI}(x) = P(f(x)) \geq (f(x^+) +\epsilon)</d-math>
        </p>
      </li>
    </ol>
    <p>
      Looking closely, essentially we are finding a value from the CDF of the probability distribution at each location. If our surrogate is a GP we can obtain a closed form solution, making the operation to calculate <d-math>\alpha_{PI}</d-math> cheap.
    </p>
    <d-math block>x_{t+1} = argmax_x \Phi\left(\frac{\mu_t(x) - f(x^+) - \epsilon}{\sigma_t(x)}\right)</d-math>
    <p>
      where <d-math>\Phi(\cdot)</d-math> indicates the CDF.
    </p>

    <h4>Intuition behind <d-math>\epsilon</d-math> in PI</h4>

    <p>
      PI makes use of <d-math>\epsilon</d-math> in order to strike a balance between exploration and exploitation. Essentially by setting <d-math>\epsilon</d-math> to some value, we want to choose a point such that the probability of improvement over <d-math>f(x^+) + \epsilon</d-math> is the most.
    </p>
    <p>
      In our following plot, we have tried to visualize how changing the value of<d-math>\epsilon</d-math> affects the values of our acquisition function, resulting in different choices of points to evaluate. We selected two points as our candidate points <d-math>x \in \{1.0, 5.0\}</d-math> to compare against each other. Changing the value of <d-math>\epsilon</d-math> (hovering our mouse), we can see that we implicitly place more importance to the uncertainty in the posterior of our surrogate. Increasing <d-math>\epsilon</d-math> results in the locations with a larger <d-math>\sigma</d-math> to benefit as their probability density is spread more. Thus points with more <d-math>\sigma</d-math> would have a higher <d-math>\alpha_{PI}</d-math> when the <d-math>\epsilon</d-math> is set to a large value.
    </p>

    <p style="font-size: 18px;">
      Feel free to play around with our <a href="#Teaser1">below</a> plot.
    </p>

    <div class="l-screen shaded-figure">
        <div>
          <text id="plotHead">
            The plot below shows how modifying ϵ leads to different points (locations) being selected to evaluate (drill) next. <br> This selection is based on the maximal probability of getting better gold content over the previous evaluations.
          </text>
        </div>
        <d-figure>
          <div class="Teaser1" id="Teaser1"></div>
          <div class="Teaser1" id="TeaserL1"></div>
        </d-figure>
        <d-figure>
          <div class="Teaser2" id="Teaser2"></div>
          <div class="Teaser2" id="TeaserL2"></div>
        </d-figure>
      </div>

    

    <p style="padding-top: 3em;">
      Now we possess the intuition behind how the Probability of Improvement is calculated. Let us now change <d-math>\epsilon</d-math> and look at its effects.
    </p>

    <figure>
      <d-figure><img src="images/MAB_gifs/mab-gp-pi-eps0.01.gif" /></d-figure>
    </figure>
    <p>
      Looking at the graph above we can see that we are not effectively exploring at value <d-math>\epsilon = 0.01</d-math> for the Probability of Improvement acquisition function. We are stuck.
    </p>

    <figure>
      <d-figure><img src="images/MAB_gifs/mab-gp-pi-eps0.5.gif" /></d-figure>
    </figure>
    <p>
      Looking above, we see increasing the value to <d-math>\epsilon = 1</d-math>, enables us to explore more and get near to the maximum value. One can notice that the
      surrogate possess large uncertainity at <d-math>x \in [2, 3.5]</d-math> (can be identified by the grey translucent area).
      But our aim in BO is to get to the maximum value, in contrast to estimation of <d-math>f(x)</d-math> in the case of active learning. Therefore, we see these points are not evaluated, using PI as our acquisition function with given hyperparameters!
    </p>

    <p>
      Let us look at what happens if we increase the hyperparameter <d-math>\epsilon</d-math> a bit more.
    </p>

    <figure>
      <d-figure><img src="images/MAB_gifs/mab-gp-pi-eps3.gif" /></d-figure>
    </figure>
    <p>
      We see that we made things worse! Our model now uses <d-math>\epsilon = 8</d-math>, which has effectively resulted in way too much exploration. This amount of exploration is not able to exploit when we land somewhere near a global maximum.
    </p>

    <h3 id="expectedimprovementei">Expected Improvement (EI)</h3>

    <p>
      Probability of improvement only looked at <em>how likely</em> is an improvement, but, shouldn't we be looking into <em>how much</em> we can improve? The next criterion called, Expected
      Improvement, (EI) does exactly that!
    </p>

    <p>
      In this acquisition function, <d-math>t + 1^{th}</d-math> query point, <d-math>x_{t+1}</d-math>, is selected according to the equation below.
    </p>
    <d-math block>
      x_{t+1} = argmin_x \mathbb{E} \left( ||h_{t+1}(x) - f(x^\star) || \ | \ \mathcal{D}_t \right)
    </d-math>
    <p>
      Where, <d-math>f</d-math> is the actual ground truth function, <d-math>h_{t+1}</d-math> is our GP posterior of the ground truth for <d-math>t+1^{th}</d-math> timestep, <d-math>\mathcal{D}_t</d-math> is the training data <d-math>\{(x_i,
        f(x_i))\} \ \forall x \in x_{1:t}</d-math> and <d-math>x^\star</d-math> is the actual position where the ground truth
      function takes the maximum value.
    </p>

    <p>
      In essence, we are trying to select the point that minimizes the distance to the objective evaluated at the maximum. Unfortunately, we don't know the ground truth function, <d-math>f</d-math>. Mockus<d-cite key="mockusEI"></d-cite> proposed
      the following acquisition function to overcome the issue.
    </p>

    <d-math block>
      x_{t+1} = argmax_x \mathbb{E} \left( {max} \{ 0, \ h_{t+1}(x) - f(x^+) \} \ | \ \mathcal{D}_t \right)
    </d-math>

    <p>
      Where <d-math>f(x^+)</d-math> is the maximum value that has been encountered so far. This equation for the case of GP surrogate, can be easily converted analytically to a closed form equation shown below.
    </p>

    <d-math block>
      EI(x)=
      \begin{cases}
      (\mu_t(x) - f(x^+) - \epsilon)\Phi(Z) + \sigma_t(x)\phi(Z), & \text{if}\ \sigma_t(x) > 0 \\
      0 & \text{if}\ \sigma_t(x) = 0
      \end{cases}
    </d-math>
    <d-math block>Z= \frac{\mu_t(x) - f(x^+) - \epsilon}{\sigma_t(x)}</d-math>
    <p>
      where <d-math>\Phi(\cdot)</d-math> indicates CDF and <d-math>\phi(\cdot)</d-math> indicates pdf.
    </p>
    <p>We can see when our <em>Expected Improvement</em> will be high.</p>

    <ul>
      <li>It is high when the expected value of <d-math>\mu_t(x) - f(x^+)</d-math> is high.</li>

      <li>It is high when the uncertainty <d-math>\sigma_t(x)</d-math> around a point is high.</li>
    </ul>

    <p>Now, if we see the role of <d-math>\epsilon</d-math> in <em>Expected Improvement</em>, it is precisely the same as in the case of <em>Probability of Improvement</em> (we have the same expression in PI) <d-footnote>A good
        introduction to the Expected Improvement acquisition function is by <a href="https://thuijskens.github.io/2016/12/29/bayesian-optimisation/" target="_blank">this post</a> by Thomas Huijskens and <a
          href="https://people.orie.cornell.edu/pfrazier/Presentations/2018.11.INFORMS.tutorial.pdf" target="_blank">these slides</a> by Peter Frazier</d-footnote>.
    </p>

    <p>
      Like the Probability of Improvement's acquisition function, we can moderate the amount of explorability the Expected Improvement's acquisition function by setting the <d-math>\epsilon</d-math> hyperparameter.
    </p>

    <figure>
      <d-figure><img src="images/MAB_gifs/mab-gp-ei-eps0.01.gif" /></d-figure>
    </figure>

    <p>
      We see that having <d-math>\epsilon = 0.01</d-math> primarily results in exploitation, and we are not able to get to the global maxima due to this myopic drilling point selection.
    </p>

    <p>
      Let us try increasing the <d-math>\epsilon</d-math> variable to focus a little more on explorability.
    </p>

    <figure>
      <d-figure><img src="images/MAB_gifs/mab-gp-ei-eps1.5.gif" /></d-figure>
    </figure>
    <p>
      As we expected, increasing the value to <d-math>\epsilon = 1.5</d-math> makes the acquisition function explore more and exploit when the time comes. We see that it moves slowly once it reaches near the global maxima,
      trying
      to find the global maxima. In this case, the exploration is effectively helping us reach a higher functional value much earlier!
    </p>

    <p>
      Let us see if increasing <d-math>\epsilon</d-math> helps us more!
    </p>

    <figure>
      <d-figure><img src="images/MAB_gifs/mab-gp-ei-eps3.gif" /></d-figure>
    </figure>
    <p>
      Is this better than before? Turns out a yes and a no. We see that here we do too much exploration given the value of <d-math>\epsilon = 3</d-math>. Which results in early reaching something close to global maxima, but unfortunately we don't
      exploit to get more gains near the global maxima. We would have liked an acquisition function that tried to exploit a bit more after reaching somewhere close to the global maxima.
    </p>

    <h4 class="collapsible">PI vs. EI</h4>
    <div class="content">
      <p>
        We have seen two closely related methods, The <em>Probability of Improvement</em> and the <em>Expected Improvement</em>.
      </p>

      <p>
        In the figure below, we have plotted the values for both policies' acquisition function's values below, for each of the possible locations. The graph shows the relation followed
        between
        EI and PI for when we have a single training point <d-math>(0.5, f(0.5))</d-math>.
      </p>

      <figure class="smaller-img">
        <d-figure><img src="images/MAB_gifs/Ei_Pi_graph/0.svg" /></d-figure>
      </figure>
      <p>
        We can see that the <d-math>\alpha_{EI}</d-math> has its maximum value corresponding to <d-math>\alpha_{PI}</d-math> close to 0.25. But, <d-math>\alpha_{PI}</d-math> has a maximum of around 0.375, for which <d-math>\alpha_{EI}</d-math>
        is low. Choosing a point with low <d-math>\alpha_{PI}</d-math> and high <d-math>\alpha_{EI}</d-math> means taking more risk (probability of improvement is low), but getting a high reward (expected improvement is high).
        In other words, multiple points have same (<d-math>\alpha_{EI}</d-math>), we should prioritize to choose the option with lesser risk (higher <d-math>\alpha_{PI}</d-math>). And similarly, when the risks are the same (same <d-math>\alpha_{PI}</d-math>) for any two points, we would want to go with the point with greater reward (higher <d-math>\alpha_{EI}</d-math>).
      </p>
    </div>

    <h3 id="thompsonsampling">Thompson Sampling</h3>

    <p>One more acquisition function that is quite common is Thompson Sampling. It has a low overhead of setting up therefore quite useful sometimes.</p>

    <p>
      The idea is to sample functions within upper and lower probabilistic bounds of a regressor. In other words, we sample functions from the surrogate's posterior. Once we have sampled a function <d-math>m_t</d-math> for the <d-math>t^{th}
      </d-math> timestep, we choose point <d-math>x_{t+1}</d-math> that maximizes the function <d-math>m_t</d-math> as the next query point to be evaluated.
    </p>

    <p>
      The intuition behind Thompson sampling can be understood by clearly noticing two important observations.
      <ul>
        <li>
          <p>
            Locations <d-math>x</d-math> with high variance (<d-math>\sigma(x)</d-math>) will show a larger variance in the functional value of the sampled function. This might help in exploration as high variation might lead to having the sampled functions more prone to having high value at locations with high variance. This will ensure an <strong>exploratory</strong> behaviour.
          </p>
        </li>
        <li>
          <p>
            The current max value evaluated be denoted by <d-math>f(x^+)</d-math>. The sampled functions will need to pass through or closely from the current max value (due to low <d-math>\sigma(x)</d-math>) at the evaluated locations. This will
            ensure an <strong>exploiting</strong> behaviour of the acquisition function.
          </p>
        </li>
      </ul>
      <p>
        Combining the above two observations, the acquisition function would accordingly do explorations and exploitation.
      </p>

      <figure>
        <d-figure><img src="images/MAB_gifs/mab-gp-thomp.gif" /></d-figure>
      </figure>

      <h3>Random</h3>

      <p>
        We have been using intelligent acquisition functions until now. 
        If we were to chose our evaluation positions <d-math>x</d-math> 
        randomly, we effectively will have changed our acquisition function
        to a Random acquisition function. This effectively is how 
        Random search works.
      </p>

      <figure>
        <d-figure><img src="images/MAB_gifs/rand.gif" /></d-figure>
      </figure>

      <h3 class="collapsible"> Other Acquisition Functions </h3>
      <div class="content">

        <p style="padding-top: 1em;">We have seen various acquisition functions until now. One trivial way to come up with acquisition functions is to have a explore/exploit combination.
        <p>

        <h3> Upper Confidence Bound (UCB) </h3>
        <p>
          One such trivial acquisition function that combines the exploration/exploitation tradeoff, is a linear combination of the mean and uncertainty of our surrogate model.
          <d-math block>\alpha(x) = \mu(x) + \lambda \times \sigma(x)</d-math>
        </p>

        <p>
          The intuition behind the UCB acquisition function is weighing of the  importance between the surrogate's mean  vs the surrogate's uncertainity. The <d-math>\lambda</d-math> above is the hyperparameter that can control the preference between exploitation or exploration.
        </p>
      

        <!--
        <figure>
          <d-figure><img src="images/MAB_gifs/acq_fn.svg" /></d-figure>
        </figure>
    
        <p>
          In the figure above, we can see an instantiation of the above-mentioned acquisition function (shown in green) for some value of <d-math>\lambda</d-math>.
          We can observe that mean near the location (predicted) of the just added point (red point) is high. Also, as we go far from the red point, we see that our uncertainty increases to a maximum. We can see that the acquisition function is low at
          the sampled point (part of the train set), as there is no uncertainty at the point. However, as we move away from the red point, our variance increases, and so does our acquisition function. But, beyond a certain value, the acquisition
          function keeps on decreasing. This is because while the variance or uncertainty is high for such points, the posterior mean is low.
    
          We see at around the location
          <d-math>x = 1.4</d-math>
          we get the maximum value for the acquisition (green curve). Thus we next select this location to drill.
        </p>
    
        <p>
          The intuition of using the acquisition function <d-math>\alpha(x) = \mu(x) + \lambda \times \sigma(x)</d-math> is that we are interested in finding the global mean <d-math>\mu</d-math>, so taking into account the estimated mean would be a
          good idea.
          Additionally, we would like to explore too (using <d-math>\sigma</d-math>); else we might be stuck in a local maxima if don't explore enough.
        </p>
    
        <p>
          Let us now try different values of <d-math>\lambda</d-math> and see the maximum gold count found by our acquisition function.
        </p>
    
        <figure>
          <d-figure><img src="images/MAB_gifs/mab-gp-1.gif" /></d-figure>
        </figure>
    
        <p>
          For <d-math>\lambda=1</d-math> as shown in the above animation, we can see that we get stuck in the local maxima. This probably means that we are not exploring enough and only exploiting near the current maxima.
        </p>
    
        <figure>
          <d-figure><img src="images/MAB_gifs/mab-gp-5.gif" /></d-figure>
        </figure>
    
        <p>
          For <d-math>\lambda=5</d-math> as shown in the above animation, we can see that we still get stuck in the local maxima. Let us now increase <d-math>\lambda</d-math> even more.
        </p>
    
        <figure>
          <d-figure><img src="images/MAB_gifs/mab-gp-10.gif" /></d-figure>
        </figure>
    
        <p>
          Perfect! We see that setting this value of <d-math>\lambda=10</d-math> resulted in finding points near the global maxima and not getting stuck in a local maximum.
        </p> -->

        <p>
          We can further form an acquisition functions by combining the existing acquisition functions. Though the physical interpretability of such combinations might not be so straightforward. One reason we might want to combine two methods is to overcome the limitations of the individual methods.
        </p>

        <h3>Probability of Improvement + <d-math>\lambda \ \times</d-math> Expected Improvement (EI-PI)</h3>

        <p>
          One such combination can be a linear combination of PI and EI.

          We know PI focuses on Probability of Improvement whereas EI focuses on the Expected Improvement. Such a combination could help in having a tradeoff between the two based on the value of <d-math>\lambda</d-math>, which can be a function of the timestep itself.
        </p>

        <h3>Gaussian Process Upper Confidence Bound (GP-UCB)</h3>

        <p>
          One another interesting acquisition function is the GP-UCB. It has the benefits of providing theoretical bounds on the convergence rates as well.<br>
          Its formulation is given by:
        </p>
    
        <d-math block>
          \alpha_{GP-UCB}(x) = \mu_t(x) + \sqrt{\mathcal{v}\tau_t}\sigma_t(x)
        </d-math>
        <p>Where <d-math>t</d-math> is the timestep and <d-math>\tau_t</d-math> is,</p>
        <d-math block>
          \tau_t = 2log \left( \frac{t^{d/2 + 2}\pi^{2}}{3\delta} \right)
        </d-math>
      <!-- TODO: UCB? am I correct to call it UCB? -->
        <p>
          We can see how this formulation looks fairly similar to the UCB acquisition function that we previously described. The difference here compared to <em>UCB</em> is that the <d-math>\lambda</d-math> here is a function of the timestep <d-math>t
          </d-math>. This helps in some of the theoretical bounds for which we would like to refer the original paper on <em>GP-UCB</em>
          <d-cite key="gpucb"></d-cite>.
        </p>
    
      <!-- <figure>
          <d-figure><img src="images/MAB_gifs/mab-gp-gp_ucb1-1.gif" /></d-figure>
        </figure>
    
        <p>
          We seem to be exploiting too much, let us increase the exploratory hyperparameters!
        </p>
    
        <figure>
          <d-figure><img src="images/MAB_gifs/mab-gp-gp_ucb3-1.gif" /></d-figure>
        </figure>
    
        <p>
          Using this set of hyperparameters, we are able to get near global maxima and further "exploit" to find the global maximum. This was a result of increasing the value of <d-math>v</d-math> to
          <d-math>3</d-math>. This shows that <d-math>v</d-math> gives weight to exploration.
        </p>
    
        <figure>
          <d-figure><img src="images/MAB_gifs/mab-gp-gp_ucb1-3.gif" /></d-figure>
        </figure>
        <p>
          Setting the values of the hyperparameters to <d-math>v = 1</d-math> and <d-math>\delta = 3</d-math> results in greater exploitation.</p>
        <figure>
          <d-figure><img src="images/MAB_gifs/mab-gp-ei_pi.gif" /></d-figure>
        </figure> 
      -->

      </div>
      <h3>Comparison</h3>

      <p>
        Below we have a graph showing a comparison of different acquisition functions<d-footnote>To know more about the difference between acquisition functions look at <a href="https://www.cs.ubc.ca/~nando/540-2013/lectures/l7.pdf">these</a> amazing
          slides from Nando De Freitas</d-footnote> discussed above. We have chosen the hyperparameters that gave us the best performance during our basic hyperparameter search.

        We had run the Random acquisition function several times with different seeds. We have plotted the mean and the standard deviation below. We can see the standard deviation (variance) is quite high, which is expected as we are randomly
        choosing points <d-math>x</d-math> to evaluate our black box function <d-math>f</d-math>.
      </p>

      <figure>
        <d-figure><img src="images/MAB_gifs/comp.svg" /></d-figure>
      </figure>

      <p>We see the <em>Random</em> method can find the maximum much before any of the other techniques (albeit the variance is high); this can be seen when we are faced with smaller spaces. But we will soon see that when we have larger search
        spaces, a random
        method
        isn't
        that efficient. This is because the probability of finding the optimum <d-math>x</d-math> in our new <d-math>f</d-math> real-valued multivariable function would drop exponentially with increasing dimensions due to what we call,
        the
        curse of dimensionality.</p>

      
      <!--
    <h2 id="higherdimensions">Higher Dimensions</h2>

    <p>For now we have been looking at real-valued single dimensional function, i.e. <d-math>f: \mathbb{R} \texttt{ -> } \mathbb{R}</d-math> data where we needed to find the value of <d-math>\boldsymbol{x}</d-math> where we reached
      global
      maximum. Let's move on and try to tackle real-valued functions of <d-math>n</d-math> real variables functions, i.e. <d-math>f: \mathbb{R}^n \texttt{ -> } \mathbb{R}</d-math>. We will soon see that our methods that we saw earlier
      for
      the
      single dimensional case can be easily ported to multi-variable functions.</p>
    -->
      <h3 id="whyisthiseasier">Why is it easier to optimize the acquisition function?</h3>

      <p>
        One valid question one might come up is that we have replaced the original optimization problem (optimizing the given black-box function) to another optimization problem (optimization of acquisition function). How is this any better than the last problem?
        
        The main reason lies in the fact that evaluating the acquisition function is much cheaper. We know that the acquisition function is a function of the posterior of the surrogate. If we have a closed-form solution, it makes the calculation of the acquisition function extremely cheap. In contrast, the original black-box function was extremely costly to evaluate, by definition.
      </p>


      <h2>Hyperparameter Tuning</h2>

      <p>Before we talk about Bayesian optimization for hyperparameter tuning<d-cite key="Snoek2012,NIPS2011_4443,Bergstra"></d-cite>, we will quickly differentiate between hyperparameters and parameters. </p>

      <p>
        Hyperparameters<d-footnote> For definitions visit the Wikipedia article on <a href="https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)">Hyperparameters</a>.</d-footnote> are parameters whose value is set before the learning
        process begins. Parameters, on the other hand, are the parameters that are learned looking at the data. One small example that we can think of can be of Ridge Regression.
      </p>
      <d-math block>
        \hat{\beta}^{ridge} = argmin_{\beta \ \in \ \mathbb{R}^p} \sum\limits_{i=1}^{n} \left(y_i - x_i^T\beta \right)^2 + \lambda \sum\limits_{i=1}^{p} \beta^2_j
      </d-math>
      <p>
        In Ridge Regression, we have the usual parameter, <d-math>\beta</d-math>: the weight matrix which is learned from the data. We further have a hyperparameter we need to set to get regularization effect, the regularization coefficient <d-math>\lambda \geq 0</d-math>. <br />
        In the case of using a Gradient Descent optimization technique  to optimize the above expression, we further introduce another hyperparameter, the learning rate <d-math>\alpha</d-math>.
      </p>

      <p>Now as we are clear on the difference between hyperparameters and parameters we would like to introduce one of the most common use case of Bayesian Optimization; <em>Hyperparameter Tuning</em>: finding the best performing
        hyperparameters
        on machine learning models. At last, hyperparameter tuning is an optimization problem (optimizing our metric of choice).</p>

      <p>Usually, when training a model isn't expensive and time-consuming, we might just do a grid search. The primary issue with grid search is that it is not feasible if getting the functional value is extremely costly, as in case of a large neural network that takes days to train. This might result in days of waiting to get the accuracy scores.

      <p>We turn to BO to counter the expensiveness of getting the functional values (accuracy values), and these increased dimensions.</p>

      <h3 id="example1">Example 1 -- Support Vector Machine</h3>

      <p>Let us use an SVM on sklearn's moons dataset and try to find the optimal hyperparameter using BO. Let us have a look at the dataset first.</p>

      <figure class="smaller-img">
        <d-figure><img src="images/MAB_gifs/moons.svg" /></d-figure>
      </figure>

      <p>
        Let us now try to learn the classification problem above using a Support Vector Machine (SVM). We are aware that an SVM takes in two important hyperparameters,
      </p>
      <p>
        <ul>
          <li>
            <d-math>\gamma</d-math> -- modifies the behavior of the SVM's kernel. Intuitively it is a measure of how far the influence of a single training example reaches<d-footnote>StackOverflow <a
                href="https://stackoverflow.com/questions/35848210/support-vector-machine-what-are-c-gamma">answer</a> for intuition behind the hyperparameters.</d-footnote>.
          </li>
          <li>
            <d-math>C</d-math> -- modifies the slackness of the classification, the higher the <d-math>C</d-math> is, the more sensitive is SVM towards the noise.
          </li>
        </ul>
      </p>
      <p>
        Let us apply Bayesian Optimization to learn the best hyperparameters for this classification task<d-footnote> <strong>Note</strong>: the surface plots you see for the Ground Truth Accuracies below were calculated for each possible hyperparameter for showcasing purposes only. We don't have these values in real applications.
        </d-footnote>. The optimum values for &#60<d-math>C, \ \gamma</d-math>&#62 have been found via running grid search at a high granularity.
      </p>

      <figure>
        <d-figure><img src="images/MAB_gifs/pi3d-0.05-mat.gif" /></d-figure>
      </figure>

      <p>Above we see a gif showing the work of the <em>Probability of Improvement</em> acquisition function in finding the best hyperparameters.</p>

      <figure>
        <d-figure><img src="images/MAB_gifs/ei3d-0.0001-mat.gif" /></d-figure>
      </figure>

      <p>Above we see a gif showing the work of the <em>Expected Improvement</em> acquisition function in finding the best hyperparameters.</p>
<!-- 
      <figure>
        <d-figure><img src="images/MAB_gifs/gp3d-1-2-mat.gif" /></d-figure>
      </figure>

      <p>
        Above we see a gif showing the work of the <em>Gaussian Processes Upper Confidence Bound</em> acquisition function in finding the best hyperparameters. This by far seems to perform the best with getting quite close to the global optimum value of hyperparameters (found using brute force).
      </p>

      <figure>
        <d-figure><img src="images/MAB_gifs/rand3d.gif" /></d-figure>
      </figure>

      <p>Now our favorite, the Random acquisition function.</p> -->

      <h3 id="comparison">Comparison</h3>

      <p>
        Below we will compare the acquisition functions in finding the best hyperparameters for our SVM model. We had again ran the Random acquisition function several times with different seeds.
      </p>

      <figure>
        <d-figure><img src="images/MAB_gifs/comp3d.svg" /></d-figure>
      </figure>

      <p>We see GP-UCB performed the best in this case. Random that was performing really nice in the last single dimensional example seems to slightly worse in this case. This can be attributed to the increase in the number of dimensions. Once there are too many dimensions, it's much difficult to get to the optimal value by using random search.</p>

      <h3 class="collapsible">Other Examples</h3>
      <div class="content">
        <h3>Example 2 -- Random Forest</h3>

        <p>
          Using Bayesian Optimization in a Random Forest Classifier.<d-cite key="scikit"></d-cite>
        </p>

        <p>
          We will continue now to train a Random Forest on the moons dataset we had used previously to learn the Support Vector Machine model. The primary hyperparameters of Random Forests we would like to optimize our accuracy are the <strong> number</strong> of
          Decision Trees we would like to have, the <strong>maximum depth</strong> for each of those decision trees.
        </p>
        <p>
          The parameters of the Random Forest are the individual trained Decision Trees models.
        </p>
        <p>
          We will be again using Gaussian Processes with Matern kernel to estimate and predict the accuracy function over the two hyperparameters.
        </p>

        <figure>
          <d-figure><img src="images/MAB_gifs/RFpi3d-0.05-mat.gif"></d-figure>
        </figure>

        <p>
          Above we see a gif showing the work of the <em>Probability of Improvement</em> acquisition function in finding the best hyperparameters.
        </p>

        <figure>
          <d-figure><img src="images/MAB_gifs/RFei3d-0.0001-mat.gif"></d-figure>
        </figure>

        <p>Above we see a gif showing the work of the <em>Expected Improvement</em> acquisition function in finding the best hyperparameters.</p>

        <figure>
          <d-figure><img src="images/MAB_gifs/RFgp3d-1-2-mat.gif"></d-figure>
        </figure>

        <p>
          Above we see a gif showing the work of the <em>Gaussian Processes Upper Confidence Bound</em> acquisition function in finding the best hyperparameters.
        </p>

        <figure>
          <d-figure><img src="images/MAB_gifs/RFrand3d.gif"></d-figure>
        </figure>

        <p>Let us now use Random acquisition function.</p>

        <figure>
          <d-figure><img src="images/MAB_gifs/RFcomp3d.svg"></d-figure>
        </figure>

        <p>
          We see that the optimization is in two dimensions. Therefore it is expected that random would perform much worse than in the case of a single dimension.
        </p>

        <h3>Example 3 -- Neural Networks</h3>
        <p>
          Let us take this example to get an idea of how to apply Bayesian Optimization to training neural networks like CNNs on Mnist. Here we will be using <d-code language="python">scikit-optim</d-code>, which also provides us support for
          optimizing
          our function on a mix of categorical, integral, and real variables. We won't be plotting the ground truth here, as it's extremely costly to do so. Below are some code snippets that go into adding Bayesian Optimization for hyperparameter
          tuning.
        </p>

        <p>
          The code below declares the search space for the optimization problem (hyperparameter tuning). In this example we are limiting the search space to be the following:
          <ul>
            <li>
              batch_size -- Our search space for the possible batch sizes consists of integer values s.t. batch_size = <d-math>2^i \ \forall \ 2 \leq i \leq 7 \ \& \ i \in \mathbb{Z}</d-math>.<br />
              This hyperparameter sets the number of training examples to combine to find the gradients for a single step in gradient descent.
            </li>
            <li>
              learning rate -- We will be searching all the real numbers from the range <d-math>[10^{-6}, \ 1]</d-math>. We will be using logarithmic uniform distribution as our prior distribution if we are to sample random points from this space.<br />
              This hyperparamter sets the stepsize with which we will perform gradient descent in the neural network.
            </li>
            <li>
              activation -- We will have one categorical variable, i.e. the activation to apply to our neural network layers. This variable can take on values in the set <d-math>\{ relu, \ sigmoid \}</d-math>.
            </li>
          </ul>
        </p>

        <d-code block language="python">
          log_batch_size = Integer(
          low=2,
          high=7,
          name='log_batch_size'
          )
          lr = Real(
          low=1e-6,
          high=1e0,
          prior='log-uniform',
          name='lr'
          )
          activation = Categorical(
          categories=['relu', 'sigmoid'],
          name='activation'
          )

          dimensions = [
          dim_num_batch_size_to_base,
          dim_learning_rate,
          dim_activation
          ]
        </d-code>

        <p>
          Moving on, we have a minimizer function imported from <d-code language="python">scikit-optim</d-code> called <d-code language="python">gp-minimize</d-code>. One can easily change the acquisition function from a number of available options.
          Below we have a code snippet showing the calling of the optimization function with the <em>Expected Improvement</em> acquisition function.
        </p>
        <p>
          <strong>Note</strong>: One will need to negate the accuracy values as we are using the minimizer function from <d-code language="python">scikit-optim</d-code>.
        </p>

        <d-code block language="python">
          # setting up default parameters (1st point)
          default_parameters = [4, 1e-1, 'relu']

          # bayesian optimization
          search_result = gp_minimize(
          func=train,
          dimensions=dimensions,
          acq_func='EI', # Expected Improvement.
          n_calls=11,
          x0=default_parameters
          )
        </d-code>

        <figure class="smaller-img">
          <d-figure><img src="images/MAB_gifs/conv.svg"></d-figure>
        </figure>

        <p>
          We now have the hyperparameters that have maximized your accuracy. In the graph above the y-axis denotes the <d-math>\left( f(x^+) \right)</d-math> and the x-axis denotes the number of times we have queried <d-math>(t)
          </d-math> the neural network with some set of given hyperparameters.
        </p>

        <p>
          We can easily apply the BO for more dimensions (more hyperparameters), even if the dimensions which are categorical (as 'relu' vs 'sigmoid' above) can easily be incorporated into BO.
        </p>

        <p>
          Looking at the above example, we can see that incorporating Bayesian Optimization isn't a big problem and saves a lot of time we can see that the network was able to get to an accuracy of nearly one in around three iterations. That's
          impressive! The example above has been inspired by Hvass Laboratories' Tutorial<d-footnote> <a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/19_Hyper-Parameters.ipynb">Notebook</a> showcasing hyperparameter
            optimization in Tensorflow. </d-footnote> on <d-code language="python">scikit-optim</d-code>.
        </p>

        <p>
          While running the experiment on our laptops, each of the evaluation cost us an approximate of 15 minutes. Looking at the number of evaluations required by the Bayesian Optimization approach, we were able to get to an accuracy of nearly 1.0 in just 5 iterations.
        </p>
        <p>
          The parameters selected by the optimizer were <d-math langugage="python">[4, 0.0019, `relu`]</d-math>, i.e., batch size of 8, learning rate of 1.9e-2 and 'relu' activation function.
        </p>
        <p>
          If we had performed a naive grid search, it would have taken us a lot more iterations <d-math>(5 \times 2 \times 7)</d-math>, and we still wouldn't have tested a learning rate near the learning rate returned by the optimizer. Suppose using grid search resulted in us to take 25 iterations. If we convert it to the time we spent over the Bayesian Optimization approach; we will get a better idea of the amount of time that can be saved when evaluation of the ground truth functions is even more costly.
        </p>

        <p>
          We would have saved around 5 hours by using BO for our hypothetical scenario of using Grid Search.
        </p>
      </div>

        <h1 id="conclusions">Conclusion and Summary</h1>

        <p>
          In this article, we looked at Bayesian Optimization, which involves optimizing a black-box function. Our primary focus is on the cases when the function evaluations are expensive, making grid or exhaustive search impractical. We looked at the key components of BO. First, we looked at the notion of using a surrogate function (with a prior over the space of objective functions) to model our black-box function. Next, we looked at the "Bayes" in BO - the function evaluations are used as data to obtain the surrogate posterior. We look at acquisition functions, which are functions of the surrogate posterior and are optimized sequentially. This new sequential optimization is in-expensive and thus of utility of us. We also looked at a few acquisition functions and showed how these different functions balance the exploration and exploitation. Finally, we looked at some practical examples of BO for optimizing hyper-parameters for machine learning models. We believe in BO, we have presented an interesting and effective suite of black-box optimization techniques. We now hope that we have whetted the reader's appetite!
        </p>

        <h2 id="embracebayesianoptimization">Embrace Bayesian Optimization</h2>

        <p>
          Optimizing or tuning hyperparams is an important facet of modern machine learning algorithms and BO is an efficient way of tuning the same.
        </p>
        <p>
          Having read all the way through, you might have been sold on the idea about the time you can save by asking Bayesian Optimizer to find the best hyperparameters for your fantastic model. There are a plethora of Bayesian Optimization libraries available. We have linked a few below. Do check them out.
        </p>

        <ul>
          <li><a href="https://scikit-optimize.github.io/">scikit-optimize</a>
            <d-footnote>Really nice tutorial showcasing hyperparameter optimization on a neural network available at this <a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/19_Hyper-Parameters.ipynb">link</a>.
            </d-footnote>
          </li>

          <li><a href="https://app.sigopt.com/docs/overview/python">sigopt</a></li>

          <li><a href="http://hyperopt.github.io/hyperopt/">hyperopt</a></li>

          <li><a href="https://github.com/HIPS/Spearmint">spearmint</a></li>

          <li><a href="https://github.com/Yelp/MOE">MOE</a></li>
        </ul>

        <p>
          We hope you had a good time reading the article and hope you are ready to <strong>exploit</strong> the power of BO. In case you wish to <strong>explore</strong> more, please read the <a href="#FurtherReading">Further Reading</a> section
          below.
        </p>

  </d-article>

  <d-appendix>

    <h3 id="FurtherReading">Further Reading</h3>
    <ol>
      <li>
        <p>Using gradient information when it is available.</p>
        <ul>
          <li>
            Suppose we have gradient information available, we should possibly try to use the information. This could result in a much faster approach to the global maxima. To know more about this exciting domain of research look at the paper by
            Wu, et. al.<d-cite key="BOwtGD"></d-cite>.
          </li>
        </ul>
      </li>
      <li>
        <p>
          To have a quick view of differences between BO and Gradient Descent, one can look at <a href="https://stats.stackexchange.com/q/161936">this</a> amazing answer at StackOverflow.
        </p>
      </li>
      <li>
        <p>
          We talked about optimizing a black-box function here. If we are to perform over multiple objectives, how do these acquisition functions scale? There has been fantastic work in this domain too! We try to deal with these cases by having multiobjective acquisition functions. Have a look at <a href="https://gpflowopt.readthedocs.io/en/latest/notebooks/multiobjective.html">this excellent</a> notebook for an example using <d-code language="python">gpflowopt</d-code>.
        </p>
      </li>
      <li>
        <p>
          One of the more interesting uses of hyperparameters optimization can be attributed to searching the space of neural network architecture for finding the architectures that give us maximal predictive performance. One might also want to consider multiobjective optimizations as some of the other objectives like memory consumption, model size, or inference time also matter in practical scenarios.
        </p>
      </li>
      <li>
        <p>
          When the datasets are extremely large, human experts tend to test hyperparameters on smaller subsets of the dataset and iteratively improve the accuracy for their models. There has been work in Bayesian Optimization, taking into account these approaches<d-cite key="hyperband,largeBO"></d-cite> when datasets are of such sizes.
        </p>
      </li>
      <li>
        <p>
          There also has been work on BO, where one explores with a certain level of "safety", meaning the evaluated values should lie above a certain security threshold functional value<d-cite key="SafeExplore"></d-cite>. One toy example is the possible configurations for a flying robot to maximize its stability. If we tried a point with terrible stability, we might crash the robot, and therefore we would like to explore the configuration space more diligently.
        </p>
      </li>
      <li>
        <p>
          We have been using GP in our BO for getting predictions, but we can have any other predictor or mean and variance in our BO.
        </p>
        <ul>
          <li>
            <p>
              One can look at <a href="http://aad.informatik.uni-freiburg.de/~hutter/ML3.pdf">this</a> slide deck by Frank Hutter discussing some limitations of a GP-based Bayesian Optimization over a Random Forest based Bayesian Optimization.
            </p>
          </li>
          <li>
            <p>
              There has been work on even using deep neural networks in BO<d-cite key="NNbasedBO"></d-cite> for a more scalable approach compared to GP. The paper talks about how GP-based Bayesian Optimization scales cubically with the number of observations, compared to their novel method that scales linearly.
            </p>
          </li>
        </ul>
      </li>
      <li>
        <p>
          Things to take care when using BO.
        </p>
        <ul>
          <li>
            <p>
              While working on the blog, we once scaled the accuracy from the range <d-math>[0, \ 1]</d-math> to <d-math>[0, \ 100]</d-math>. This changed broke havoc as the Gaussian Processes we were using had certain hyperparameters, which needed
              to be scaled with the accuracy to maintain scale invariance. We wanted to point this out as it might be helpful for the readers who would like to start using on BO.
            </p>
          </li>
          <li>
            <p>
              We need to take care while using Bayesian Optimization. Bayesian Optimization based on Gaussian Processes Regression is highly sensitive to the kernel used. For example, if you are using <a
                href="https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Matern.html">Matern</a> kernel, we are implicitly assuming that the function we are trying to optimize is first order differentiable.
            </p>
          </li>
          <li>
            <p>
              Searching for the hyperparameters, and the choice of the acquisition function to use in BO are interesting problems in themselves. There has been amazing work done, looking at this problem. As mentioned previously in the post, there has
              been work done in strategies using multiple acquisition function<d-cite key="multiACQ"></d-cite> to deal with these interesting issues.
            </p>
          </li>
          <li>
            <p>
              A nice list of tips and tricks one should have a look at if you aim to use Bayesian Optimization in your workflow is from this fantastic post by Thomas on <a href="https://thuijskens.github.io/2016/12/29/bayesian-optimisation/">Bayesian
                Optimization with sklearn</a>.
            </p>
          </li>
        </ul>
      </li>
      <li>
        <p>
          BO applications.
        </p>
        <ul>
          <li>
            <p>
              BO has been applied to Optimal Sensor Set selection for predictive accuracy<d-cite key="sensorBO"></d-cite>.
            </p>
          </li>
          <li>
            <p>
              Pater Faizer in his <a href="https://www.youtube.com/watch?v=c4KKvyWW_Xk">talk</a> mentioned that Uber uses Bayesian Optimization for tuning algorithms via backtesting.
            </p>
          </li>
          <li>
            <p>Facebook<d-cite key="letham2019"></d-cite> uses Bayesian Optimization for A/B testing.
          </li>
          <li>
            <p>
              Netflix and <a href="https://engineeringblog.yelp.com/2014/10/using-moe-the-metric-optimization-engine-to-optimize-an-ab-testing-experiment-framework.html">Yelp</a> use Metrics Optimization software like <a href="http://github.com/Yelp/MOE">Metrics Optimization Engine (MOE)</a> which take advantage of Parallel Bayesian Optimization<d-cite key="yelpBO"></d-cite>.
            </p>
          </li>
        </ul>
      </li>
    </ol>

    <d-bibliography src="references.bib"></d-bibliography>
  </d-appendix>

</body>

</html>

<!--
<figure>
  <d-figure><img src="images/MAB_gifs/active-gp.gif" /></d-figure>
</figure>

<figure>
  <d-figure><img src="images/MAB_gifs/mab-gp-pi-eps0.5.gif" /></d-figure>
</figure>

<ul>
  <li>
    <p>Here are representative animations showing the process of drilling at new locations and to reduce the uncertainty and get the best predictions showcasing the <strong>Active learning</strong> problem.</p>
  </li>

  <li>
    <p>And drilling at locations to get the location of the maximum gold reserve, showcasing the <strong>Bayesian Optimization</strong> problem.</p>
  </li>
</ul>

<p>We will build the solution to both of these problems from the ground up.</p>


<h1 id="bayesianoptimizationvsgradientdescent">Bayesian Optimization vs. Gradient Descent</h1>

<p>Some of the main differences between BO and GD as pointed out at StackExchange<d-cite key="BOvsGD"></d-cite>:</p>

<ul>
  <li>The biggest difference between Bayesian Optimization and Gradient Descent is that in the latter case, we have access to the gradient values.</li>

  <li>BO doesn't assume the function to be convex, in the case of Gradient Descent if you would like to get to the global minima, your function should be convex.</li>

  <li>BO assumes the function we are optimizing is fairly smooth.</li>

  <li>BO doesn't scale well with large data, as the GP inference is cubic in the number of points.</li>
</ul>

<p>There has been work done in Bayesian Optimization to make use of gradient information. The idea is based on the fact that the Gaussian Processes' predictions can be updated to take in the gradient information we get from the
  function
  we are trying to optimize. For more information, we would suggest looking at the paper Bayesian Optimization with Gradients<d-cite key="BOwtGD"></d-cite> from Wu et. al..</p>

<p>Now, as we have described BO more technically, let's have a look at how we can use this method in the case of Hyperparameter Tuning. Hyperparameters, you ask?</p>

</html> -->