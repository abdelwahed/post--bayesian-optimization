<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Exploring Bayesian Optimization</title>
  <script defer src="js/template.v2.js"></script>
  <link rel="stylesheet" type="text/css" href="css/styles.css">
  <script src="https://d3js.org/d3.v4.min.js"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/d3-legend/2.25.6/d3-legend.js"></script>
  <script defer src="js/plot.js"></script>
  <script defer src="js/hider.js"></script>
  <script defer src="js/gif-slider.js"></script>
  <script type="text/javascript" src="https://d3js.org/d3-scale-chromatic.v1.min.js"></script>

</head>

<body onload="appendInputButtons();">

  <d-front-matter>
    <script id='distill-front-matter' type="text/json">
      {
        "title": "Exploring Bayesian Optimization",
        "description": "Breaking Bayesian Optimization into small, sizeable chunks.",
        "authors": [{
            "author": "Apoorv Agnihotri",
            "authorURL": "https://apoorvagnihotri.github.io/",
            "affiliations": [{
              "name": "Indian Insitute of Technology Gandhinagar",
              "affiliationURL": "https://www.iitgn.ac.in/"
            }]
          },
          {
            "author": "Nipun Batra",
            "authorURL": "https://nipunbatra.github.io/",
            "affiliations": [{
              "name": "Indian Insitute of Technology Gandhinagar",
              "affiliationURL": "https://www.iitgn.ac.in/"
            }]
          }
        ],
        "katex": {
          "delimiters": [{
            "left": "$$",
            "right": "$$",
            "display": false
          }]
        }
      }
    </script>
  </d-front-matter>

  <d-title style="padding-bottom: 0">
    <p>Breaking Bayesian Optimization into small, sizeable chunks.</p>
  </d-title>

  <d-byline></d-byline>

  <d-article style="overflow-x: unset;">
    <p>
    <!-- <strike>
      We are increasingly getting used to machine learning algorithms with a large number of hyperparameters along with numerous optimization parameters. For example, neural networks with hyperparameters like the number of layers or optimization parameters like the learning rate, the dropout rate, among others. Or random forests with hyperparameters such as the number of trees, and the maximum depth of an individual tree. How do we efficiently tune these to optimize our machine learning model?
    </strike> -->
      Many modern machine learning algorithms have a large number of hyperparameters. To effectively use these algorithms, we need to pick good hyperparameter values.

      In this article, we will talk about Bayesian Optimization (BO), a suite of techniques often used to efficiently tune the hyperparameters, optimization parameters, and other model parameters. More generally, BO can be used to optimize any black-box function.
    </p>
    <p>

    </p>

    <p>
      However, before we talk in-depth about using BO to tune model parameters, we will study using BO to maximize (optimize) a black-box function.
    </p>

    <h1>Mining Gold!</h1>
    <p>
      Let us start with the example of gold mining. Our goal is to mine for gold in a new, unknown land<d-footnote>Interestingly, our example is similar to one of the first use of Gaussian Process (GP) (also called kriging)<d-cite key="goldKridge"></d-cite>, where Prof. Krige modelled gold concentrations using GPs.</d-footnote>.
      For now, let us make a simplifying assumption, the gold content lies in a one-dimensional space, i.e., we are talking about gold distribution only about a line. We want to find the location along this line with the maximum gold while only drilling a few times (as drilling is expensive).
    </p>
    <p>
      Initially, we have no idea about the gold distribution. The only way to learn it is to drill at different locations. However, this drilling is costly. Thus, we want to <strong>minimize the number of drillings required</strong> while still <strong>finding the location of maximum gold quickly</strong>.
    </p>

    <p>
      We now discuss two common objectives for the gold mining problem.
    </p>

    <ul>
      <li>
        <p>
          <strong>Problem 1: Best Estimate of Gold Distribution (Active Learning)</strong><br/>
          Here we want to estimate the amount of gold on the one-dimensional line, using a small number of drillings. We can not drill at every location due to the prohibitive cost. Instead, we should drill at those locations that provide us with the <strong>maximum information</strong> about the distribution of the gold. This problem is akin to
          <strong>
            active learning<d-cite key="settles2009active,Tong2001"></d-cite>
          </strong>.
        </p>
      </li>

      <li>
        <p>
          <strong>Problem 2: Location of Maximum Gold (Bayesian Optimization)</strong><br/>
          Here we want to find the location in the one-dimensional space where the gold quantity is the maximum, using a small number of drillings. This problem is akin to
          <strong>
            Bayesian Optimization<d-cite key="humanOut,nandoBOtut"></d-cite>
          </strong> (BO).
        </p>
      </li>
    </ul>

    <p>
      We will soon see how the two problems are related, but not the same.
    </p>



    <h2>Active Learning</h2>

    <p>
      For many machine learning problems, unlabeled data is readily available. However, labeling (or querying) could be an expensive task which one would like to minimize. As an example, for a speech-to-text task, the annotation requires expert(s) to label words and sentences manually. More often, this is a time consuming and expensive task. In our gold mining problem, drilling (akin to labeling) is an expensive operation. Active learning provides a way to minimize labeling while maximizing modeling accuracy. While there are various methods in the active learning literature, we will only look at <strong>uncertainty reduction</strong>. This method chooses the most uncertain point as the next query point. Often, the variance acts as a measure of uncertainty.
    </p>

    <h3>Surrogate Model</h3>
    <p>
      Active learning (along with BO, which we will see later) employs a surrogate model for modeling the unknown true function <d-math>f(x)</d-math>. The surrogate model ideally models the true function closely. In our example, <d-math>f(x)</d-math> denotes the true gold content on our new land, and unquestionably we do not know <d-math>f(x)</d-math>.
    </p>

    <h3>Bayesian Update</h3>
    <p>
      Every evaluation (drilling) of <d-math>f(x)</d-math> gives the surrogate model more data to learn. The posterior for the surrogate is obtained using the Bayes Rule with this new data at every iteration. At the end of an iteration, the posterior becomes the prior for the next cycle.
    </p>

    <p>
      Commonly, surrogates employ Gaussian Processes. One can set priors of a Gaussian Process (GP) by using specific kernels and mean functions. Moreover, GPs provide predictions as well as uncertainty estimates. We leverage these quantities in both active learning and BO.
    </p>

    <h3>Gaussian Processes</h3>

    <p>
      One might want to look at this excellent Distill article<d-cite key="gÃ¶rtler2019a"></d-cite> on Gaussian Processes<d-cite key="Rasmussen2004"></d-cite>.
      We will use Gaussian Process Regression to model the gold distribution.
    </p>

    <p>
      Let us visualize our true function <d-math>f(x)</d-math>. The gold distribution in our data is bi-modal, with a maximum value around <d-math>x = 5</d-math>. For now, let us not worry about the X-axis or the Y-axis units.
    </p>
    <figure class="smaller-img">
      <d-figure><img src="images/MAB_gifs/GT.svg" /></d-figure>
    </figure>


    <h4 id="priormodel">Prior Model</h4>

    <p>
      We define a prior over a set of functions based on our initial beliefs of the black-box function<d-cite key="Rasmussen2004"></d-cite>. The prior tries to capture the properties of the black-box function, which include periodicity, smoothness, among others. In our case, we consider the gold distribution to be smooth. We use kernels to set a prior that favors smooth functions. <d-footnote>We use a Matern 5/2 kernel due to its property of favoring doubly differentiable functions. In contrast, Matern 3/2 favors singly differentiable functions.

      See <a href="http://www.gaussianprocess.org/gpml/">Rasmussen and Williams 2004</a> and <a href="https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Matern.html">scikit-learn</a>, for details regarding the Matern kernel.
    </d-footnote> The black line and the grey shaded region indicate the mean (<d-math>\mu</d-math>) and uncertainty <d-footnote> Technically we plotted (<d-math>\mu \pm \sigma </d-math>) </d-footnote> in our gold distribution estimate before drilling.

    </p>

    <figure>
      <d-figure><img src="images/MAB_gifs/prior.svg" /></d-figure>
    </figure>

    <h4 id="addingtrainingdata">Adding Training Data</h4>

    <p>
      Let us now add the point <d-math>< x = 0.5, \ y = f(0.5) ></d-math> to the training set.
    </p>

    <figure>
      <d-figure><img src="images/MAB_gifs/posterior.svg" /></d-figure>
    </figure>

    <p>
      We see our surrogate's changed posterior <d-footnote>Shown as the prediction</d-footnote> is conveying the certainty in gold content near <d-math>x = 0.5</d-math>. Moreover, the predicted gold concentration of points near <d-math>x = 0.5</d-math> is close to the actual value that we got from drilling.<!-- <d-footnote>We can transform the data and fit a GP over <d-math>\log\left(f(x)\right)</d-math> instead of <d-math>f(x)</d-math> to ensure the  predictions are non-negative</d-footnote> . Having discussed the Bayesian update for GP regression, we will discuss the key idea of active learning: -->
    </p>

    <h3 id="activelearningprocedure">Active Learning Procedure
    </h3>

    <ol>
      <li>Choose and add the point with the highest uncertainty to the training set (by querying/labeling that point)</li>
      <li>Train on the new training set</li>
      <li>Go to #1 till convergence or budget elapsed</li>
    </ol>

    <p>
      Let us now visualize this process and see how our posterior changes at every iteration (after each drilling).
    </p>
    <figure class="gif-slider">
      <d-figure><img src="images/active-gp-img/0.png"/></d-figure>
    </figure>


    <p>
      Choosing the most uncertain location leads to querying of the points that are the farthest from the current set of train points. The visualization clears that one can estimate the true distribution in a few iterations. At every iteration, active learning <strong> explores
      </strong> the domain to make the estimates better.
    </p>

    <h2 id="bayesianoptimization">Bayesian Optimization</h2>
    <p>
      In this problem we aim to find the location of maximum gold content. The setting is similar to problem 1, but the objectives are different. Bayesian Optimization (BO) is an optimization technique that solves problem 1, i.e., maximizing a black-box function, whereas active learning focuses on getting a good estimate of that black-box function.
    </p>

    <p>
      One way to find the maximum would be to first run active learning and then select the point giving the maximum. However, in a sense, we waste evaluations to improve the estimates even though we are only concerned with finding the point giving the maximum. Assuming that our black-box function is smooth, it might be a good idea to evaluate at or near locations where our surrogate model's prediction is the highest, i.e., to <strong>exploit</strong>. However, due to the limited evaluations, our model's predictions are inaccurate as well. One can improve the model by evaluating at points with high variance or performing <strong>exploration</strong>. BO combines <strong>exploitation</strong> and <strong>exploration</strong>, whereas active learning solely <strong>explores</strong>.
    </p>

    <h1>Formalizing Bayesian Optimization</h1>

      <span style="padding-bottom: 1em;">
        Let us now formally introduce Bayesian Optimization. Our goal is to find the location (<d-math>{x \in \mathbb{R}^d}</d-math>) corresponding to the global maximum (or minimum) of a function <d-math>f: \mathbb{R}^d \mapsto \mathbb{R}</d-math>.
        We present the general constraints <d-footnote>based on the slides/talk from Peter Fraizer at Uber on Bayesian Optimization:<br>
          <ul>
            <li> <a href="https://www.youtube.com/watch?v=c4KKvyWW_Xk">Youtube Talk</a></li>
            <li> <a href="https://people.orie.cornell.edu/pfrazier/Presentations/2018.11.INFORMS.tutorial.pdf">Slide Deck</a></li>
          </ul>
        </d-footnote> in BO and contrast them with the constraints in our gold mining example.</span>




      <table>
        <tr>
          <th><h3>General Constraints</h3></th>
          <th><h3>Constraints in Gold Mining example</h3></th>
        </tr>
        <tr>
          <td><d-math>f</d-math>âs feasible set <d-math>A</d-math> is simple,
          e.g., box constraints.</td>
          <td>Our domain in the gold mining problem is a single-dimensional box constraint: <d-math>0 \leq x \leq 6</d-math>.</td>
        </tr>
        <tr>
          <td><d-math>f</d-math> is continuous but lacks special structure,
          e.g., concavity, that would make it easy to optimize.</td>
          <td>Our true function is neither convex nor concave function resulting in local optimas.</td>
        </tr>
        <tr>
          <td><d-math>f</d-math> is derivative-free:
          evaluations do not give gradient information.</td>
          <td>Our evaluation (by drilling) of the amount of gold content at a location did not give us any gradient information.</td>
        </tr>
        <tr>
          <td><d-math>f</d-math> is expensive to evaluate:
          the number of times we can evaluate it
          is severely limited.</td>
          <td>Drilling is costly.</td>
        </tr>
        <tr>
          <td><d-math>f</d-math> may be noisy. If noise is present, we will assume it is independent and normally distributed, with common but unknown variance.</td>
          <td>We assume noiseless measurements in our modeling (though, it is easy to incorporate normally distributed noise for GP regression).</td>
        </tr>
      </table>

      <p>
        Our gold mining problem is suited to use BO. Let us introduce some additional topics before you run to get the maximal gold for yourself!
      </p>

    <h3>Acquisition Functions</h3>

    <p>
      Our original optimization problem, <d-math>x^* = \text{argmax}_{x \in A} f(x)</d-math> is hard because <d-math>f</d-math> is <b>expensive</b> to evaluate.
      The idea of BO is to <b>transform</b> the original optimization into a <b>sequence</b> of easier <b>inexpensive</b> optimizations of functions called an <b>acquisition functions</b> (<d-math>\alpha(x)</d-math>).
      Intuitively, acquisition functions are heuristics<d-footnote>https://botorch.org/docs/acquisition</d-footnote> that evaluate the utility of a point for maximizing the underlying black-box function (<d-math>f(x)</d-math>)<d-footnote>Please find <a href="https://www.cse.wustl.edu/~garnett/cse515t/spring_2015/files/lecture_notes/12.pdf">these</a> slides from Washington University in St. Louis to know more and the following </d-footnote>.
      At each step, we optimize the acquisition function to determine the next point to sample.
    </p>

    <p>
      Let us re-wind and link the things discussed thus far, by noting the steps of BO<d-footnote>Please find <a href="https://youtu.be/EnXxO3BAgYk">this</a> amazing video from Javier GonzÃ¡lez at The Gaussian Process Summer School 2019.</d-footnote> and explicitly highlighting the "Bayesian" in BO.
    </p>
    <p>
      <ol>
        <li>
          We first choose a surrogate model for modeling the true function <d-math>f</d-math> and define its <b>prior</b>
        </li>
        <li>
          Given the set of <b>observations</b> (function evaluations), use Bayes rule to obtain the <b>posterior</b>.
        </li>
        <li>
          Use an acquisition function <d-math>\alpha(x)</d-math>, which is a function of the posterior, to decide the next sample point <d-math>x_t = \text{argmax}_x \alpha(x)</d-math>.
        </li>
        <li>
          Add newly sampled data to the set of <b>observations</b> and goto Step #2 till convergence or budget elapses.
        </li>
      </ol>
    </p>

    <p>
      Thus, the "Bayesian" in BO is sequentially refining our surrogate's posterior (and thus uncertainty) with each evaluation via Bayesian posterior updating<d-cite key="nandoBOLoop"></d-cite>.
    </p>
    <p>Let us now look at a few common acquisition functions.</p>

    <h3>Probability of Improvement (PI)</h3>

    <p>
      In this acquisition function, we evaluate the point with the highest probability of improvement over the current max <d-math>f(x^+)</d-math>, where <d-math> x^+ = \text{argmax}_{x_i \in x_{1:t}}f(x_i)</d-math> and <d-math>x_i</d-math> is the location queried at <d-math>i^{th}</d-math> time step.
      This acquisition function essentially is the left tail probability of the surrogate posterior.
    </p>

    <ol>
      <li>
        <p>
          Let <d-math>f(x^+)</d-math> be the current highest value of the function
        </p>
      </li>
      <li>
        <p>
          Let <d-math>\epsilon</d-math> be close to zero
        </p>
      </li>

      <li>
        <p>
          Choose <d-math>x_{t+1} = argmax(\alpha_{PI}(x))</d-math> where <d-math>\alpha_{PI}(x) = P(f(x)) \geq (f(x^+) +\epsilon)</d-math>
        </p>
      </li>
    </ol>
    <p>
      Looking closely, essentially we are finding a value from the CDF of the probability distribution at each location. If our surrogate is a GP we can obtain an analytical expression, enabling cheap calculation of <d-math>\alpha_{PI}</d-math>.
    </p>
    <d-math block>x_{t+1} = argmax_x \Phi\left(\frac{\mu_t(x) - f(x^+) - \epsilon}{\sigma_t(x)}\right)</d-math>
    <p>
      where <d-math>\Phi(\cdot)</d-math> indicates the CDF.
    </p>

    <p class='check'>
      Below is an image.
    </p>

    <figure>
      <d-figure><img src="images/MAB_gifs/pi_cdf.png" /></d-figure>
    </figure>

    <h4>Intuition behind <d-math>\epsilon</d-math> in PI</h4>

    <p>
      PI uses <d-math>\epsilon</d-math> to strike a balance between exploration and exploitation. <!-- In the following plot, we visualize how changing the value of <d-math>\epsilon</d-math> affects the values of our acquisition function, resulting in different choices of points to evaluate. We selected two points as our candidate points <d-math>x \in \{1.0, 5.0\}</d-math>, and show their tail probabilities in shaded green and orange, respectively. -->
      Increasing <d-math>\epsilon</d-math> results in the locations with a larger <d-math>\sigma</d-math> to have higher <d-math>\alpha_{PI}</d-math> as their probability density is spread more due to higher uncertainty.
    </p>

    <!-- <div class="l-screen shaded-figure">
        <div>
          <text id="plotHead">
            The plot below shows how modifying Ïµ leads to different points (locations) being selected to evaluate (drill) next. <br> This selection is based on the maximal probability of getting better gold content over the previous evaluations.
          </text>
        </div>
        <d-figure>
          <div class="Teaser1" id="Teaser1"></div>
          <div class="Teaser1" id="TeaserL1"></div>
        </d-figure>
      </br>
        <d-figure>
          <div class="Teaser2" id="Teaser2"></div>
          <div class="Teaser2" id="TeaserL2"></div>
        </d-figure>
      </div> -->

    <p>
      Now that we possess the intuition behind <d-math>\epsilon</d-math>, let us see the effects of changing it.
    </p>

    <figure class="gif-slider">
      <d-figure><img src="images/MAB_pngs/PI/0.01/0.png" /></d-figure>
    </figure>
    <p>
      Looking at the graph above<d-footnote>Ties are broken randomly.</d-footnote>, we see that we reach the global maxima in a few iterations.
      Our surrogate possesses a large uncertainty in <d-math>x \in [2, 4]</d-math><d-footnote>This can be identified by the grey translucent area.</d-footnote> in the first eight-nine iterations.
      The acquisition function initially exploits regions with high promise<d-footnote>Points in the vicinity of current maxima</d-footnote> which leads to a high uncertainty in the region <d-math>x \in [2, 4]</d-math>. This observation also shows that we do not need to construct an accurate estimate of the black-box function to find its maximum.
    </p>

    <figure class="gif-slider">
      <d-figure><img src="images/MAB_pngs/PI/0.3/0.png" /></d-figure>
    </figure>
    <p>
      The visualization above shows that increasing <d-math>\epsilon</d-math> to 0.3, enables us to explore more. However, it seems that we are exploring more than required.
    </p>

    <p>
      What happens if we increase <d-math>\epsilon</d-math> a bit more?
    </p>

    <figure class="gif-slider">
      <d-figure><img src="images/MAB_pngs/PI/3/0.png" /></d-figure>
    </figure>
    <p>
      We see that we made things worse! Our model now uses <d-math>\epsilon = 3</d-math>, which has effectively resulted in way too much exploration. This amount of exploration is not able to exploit when we land near the global maximum.
    </p>

    <p> Our quick experiments above help us conclude that <d-math>\epsilon</d-math> controls the degree of exploration in the PI acquisition function.

    <h3 id="expectedimprovementei">Expected Improvement (EI)</h3>

    <p>
      Probability of improvement only looked at <em>how likely</em> is an improvement, but, did not consider <em>how much</em> we can improve? The next criterion, called Expected Improvement (EI), does exactly that<d-footnote>A good
        introduction to the Expected Improvement acquisition function is by <a href="https://thuijskens.github.io/2016/12/29/bayesian-optimisation/" target="_blank">this post</a> by Thomas Huijskens and <a
          href="https://people.orie.cornell.edu/pfrazier/Presentations/2018.11.INFORMS.tutorial.pdf" target="_blank">these slides</a> by Peter Frazier</d-footnote>!
      The idea is fairly simple - choose the next point as the one which has the highest expected improvement over the current max <d-math>f(x^+)</d-math>, where <d-math> x^+ = \text{argmax}_{x_i \in x_{1:t}}f(x_i)</d-math> and <d-math>x_i</d-math> is the location queried at <d-math>i^{th}</d-math> time step.
    </p>

    <p>
      In this acquisition function, <d-math>t + 1^{th}</d-math> query point, <d-math>x_{t+1}</d-math>, is selected according to the following equation.
    </p>
    <d-math block>
      x_{t+1} = argmin_x \mathbb{E} \left( ||h_{t+1}(x) - f(x^\star) || \ | \ \mathcal{D}_t \right)
    </d-math>
    <p>
      Where, <d-math>f</d-math> is the actual ground truth function, <d-math>h_{t+1}</d-math> is the posterior mean of the surrogate at <d-math>t+1^{th}</d-math> timestep, <d-math>\mathcal{D}_t</d-math> is the training data <d-math>\{(x_i,
        f(x_i))\} \ \forall x \in x_{1:t}</d-math> and <d-math>x^\star</d-math> is the actual position where <d-math>f</d-math> takes the maximum value.
    </p>

    <p>
      In essence, we are trying to select the point that minimizes the distance to the objective evaluated at the maximum. Unfortunately, we do not know the ground truth function, <d-math>f</d-math>. Mockus<d-cite key="mockusEI"></d-cite> proposed
      the following acquisition function to overcome the issue.
    </p>

    <d-math block>
      x_{t+1} = argmax_x \mathbb{E} \left( {max} \{ 0, \ h_{t+1}(x) - f(x^+) \} \ | \ \mathcal{D}_t \right)
    </d-math>

    <p>
      where <d-math>f(x^+)</d-math> is the maximum value that has been encountered so far. This equation for the case of GP surrogate, can be converted an analytical expression shown below.
    </p>

    <d-math block>
      EI(x)=
      \begin{cases}
      (\mu_t(x) - f(x^+) - \epsilon)\Phi(Z) + \sigma_t(x)\phi(Z), & \text{if}\ \sigma_t(x) > 0 \\
      0 & \text{if}\ \sigma_t(x) = 0
      \end{cases}
    </d-math>
    <d-math block>Z= \frac{\mu_t(x) - f(x^+) - \epsilon}{\sigma_t(x)}</d-math>
    <p>
      where <d-math>\Phi(\cdot)</d-math> indicates CDF and <d-math>\phi(\cdot)</d-math> indicates pdf.
    </p>
    <p>From the above expression, we can see that <em>Expected Improvement</em> will be high when the expected value of <d-math>\mu_t(x) - f(x^+)</d-math> is high, or when the uncertainty <d-math>\sigma_t(x)</d-math> around a point is high.


    <p> Like the Probability of Improvement's acquisition function, we can moderate the amount of explorability of the Expected Improvement's acquisition function by setting the <d-math>\epsilon</d-math> hyperparameter.
    </p>

    <figure class="gif-slider">
      <d-figure><img src="images/MAB_pngs/EI/0.01/0.png" /></d-figure>
    </figure>

    <p>
      For <d-math>\epsilon = 0.01</d-math> we come close to the global maxima in a few iterations.  </p>

    <p>
      We now increase <d-math>\epsilon</d-math> to explore more.
    </p>

    <figure class="gif-slider">
      <d-figure><img src="images/MAB_pngs/EI/0.3/0.png" /></d-figure>
    </figure>
    <p>
      As we expected, increasing the value to <d-math>\epsilon = 0.3</d-math> makes the acquisition function explore more. Compared to the earlier evaluations, we see less exploitation. We see that it evaluates only two points near the global maxima.
    </p>

    <p>
      Let us increase <d-math>\epsilon</d-math> even more.
    </p>

    <figure class="gif-slider">
      <d-figure><img src="images/MAB_pngs/EI/3/0.png" /></d-figure>
    </figure>
    <p>
      Is this better than before? It turns out a yes and a no. We see that here we do too much exploration, given the value of <d-math>\epsilon = 3</d-math>. This results in early reaching something close to global maxima, but unfortunately, we do not exploit to get more gains near the global maxima.
    </p>

    <h4 class="collapsible">PI vs. EI</h4>
    <div class="content">
      <p>
        We have seen two closely related methods, The <em>Probability of Improvement</em> and the <em>Expected Improvement</em>.
      </p>

      <p>
        In the figure below, we make a scatter plot showing the policies' acquisition functions evaluated on different points in the domain (each dot is a point in the domain). In this plot, our train set consists of a single point <d-math>(0.5, f(0.5))</d-math>.
      </p>

      <figure class="smaller-img">
        <d-figure><img src="images/MAB_gifs/Ei_Pi_graph/0.svg" /></d-figure>
      </figure>
      <p>
        We can see that the <d-math>\alpha_{EI}</d-math> and <d-math>\alpha_{PI}</d-math> reach a maximum of 0.3 and around 0.44, respectively. Choosing a point with low <d-math>\alpha_{PI}</d-math> and high <d-math>\alpha_{EI}</d-math> is high risk (probability of improvement is low) and high reward (expected improvement is high).
        If multiple points have the same (<d-math>\alpha_{EI}</d-math>), we should prioritize the point with lesser risk (higher <d-math>\alpha_{PI}</d-math>). Similarly, when the risks are the same (same <d-math>\alpha_{PI}</d-math>) for any two points, we should choose the point with greater reward (higher <d-math>\alpha_{EI}</d-math>).
      </p>
    </div>

    <h3 id="thompsonsampling">Thompson Sampling</h3>

    <p>
      Another common acquisition function is Thompson Sampling <d-cite key="thompson"></d-cite>. At every step, we sample a function from the surrogate's posterior and optimize the sampled function. For example, in the case of the gold mining, weâd sample a plausible distribution of the gold given the evidence we have so far and evaluate wherever it peaks.
    </p>

    <p>
      Below we have an image showing three sampled functions from the learned surrogate posterior for our gold mining problem. The training data constituted the point <d-math>x = 0.5</d-math> and the corresponding functional value.
    </p>

    <figure>
      <d-figure>
        <img src="images/MAB_gifs/thompson.svg" />
      </d-figure>
    </figure>

    <p>



      The intuition behind Thompson sampling can be understood by noticing two important observations.
      <ul>
        <li>
          <p>
            Locations <d-math>x</d-math> with high variance (<d-math>\sigma(x)</d-math>) will show a larger variance in the functional values sampled from the surrogate posterior. This can aid exploration as high variation can lead to sampled functions with a high value at locations with high variance. This will ensure an <strong>exploratory</strong> behavior.
          </p>
          <p>
            As an example, the three samples (sample #1, #2, #3) show high variance close to <d-math>x=6</d-math>. Optimizing sample 3  would lead to sampling <d-math>x=6</d-math> and thus lead to exploration.
          </p>

        </li>
        <li>
          <p>
            The sampled functions will need to pass through or closely from the current max value (due to low <d-math>\sigma(x)</d-math>) at the evaluated locations. This will
            ensure an <strong>exploiting</strong> behavior of the acquisition function.
          </p>

          <p>
            As an example of this behavior, we see that all the sampled functions pass through the current max at <d-math>x = 0.5</d-math>. If <d-math>x = 0.5</d-math> was close to the global maxima then we would be able to <strong>exploit</strong> and choose a better maximum.
          </p>

        </li>
      </ul>


    <p>
      Notice that Thompson Sampling only required us to sample our posterior, not evaluate probabilities. This is a very useful property when there isnât an easy way to evaluate probabilities.
    </p>
      <p>
        We now show a visualization of the Thompson sampling acquisition function.
      </p>

      <figure class="gif-slider">
        <d-figure><img src="images/MAB_pngs/Thompson/0.01/0.png" /></d-figure>
      </figure>

      <h3>Random</h3>

      <p>
        We have been using intelligent acquisition functions until now.
        If we were to chose our evaluation positions <d-math>x</d-math>
        randomly, we effectively will have changed our acquisition function
        to a Random acquisition function.
      </p>

      <figure class="gif-slider">
        <d-figure><img src="images/MAB_pngs/Rand/0.png" /></d-figure>
      </figure>

      <h3>Summary of Acquisition Functions</h3> <p>
        Before we go ahead and provide an empirical comparison of different acquisition functions, let us summarize the core ideas associated with acquisition functions: i) they are heuristics for evaluating the utility of a point; ii) they are a function of the surrogate posterior; iii) they combine exploration and exploitation; and iv) they are inexpensive to evaluate</p>

      <h3 class="collapsible"> Other Acquisition Functions </h3>
      <div class="content">

        <p>We have seen various acquisition functions until now. One trivial way to come up with acquisition functions is to have a explore/exploit combination.
        <p>

        <h3> Upper Confidence Bound (UCB) </h3>
        <p>
          One such trivial acquisition function that combines the exploration/exploitation tradeoff is a linear combination of the mean and uncertainty of our surrogate model. The model mean signifies exploitation (of our model's knowledge) and model uncertainty signifies exploration (due to our model's lack of observations).
          <d-math block>\alpha(x) = \mu(x) + \lambda \times \sigma(x)</d-math>
        </p>

        <p>
          The intuition behind the UCB acquisition function is weighing of the  importance between the surrogate's mean  vs. the surrogate's uncertainty. The <d-math>\lambda</d-math> above is the hyperparameter that can control the preference between exploitation or exploration.
        </p>

        <p>
          We can further form acquisition functions by combining the existing acquisition functions though the physical interpretability of such combinations might not be so straightforward. One reason we might want to combine two methods is to overcome the limitations of the individual methods.
        </p>

        <h3>Probability of Improvement + <d-math>\lambda \ \times</d-math> Expected Improvement (EI-PI)</h3>

        <p>
          One such combination can be a linear combination of PI and EI.

          We know PI focuses on the probability of emprovement, whereas EI focuses on the expected improvement. Such a combination could help in having a tradeoff between the two based on the value of <d-math>\lambda</d-math>, which can be a function of the timestep itself.
        </p>

        <h3>Gaussian Process Upper Confidence Bound (GP-UCB)</h3>

        <p>
          Before talking about GP-UCB, let us quickly talk about <strong>regret</strong>. Imagine if the maximum gold was <d-math>a</d-math> units, and our optimization instead samples a location containing <d-math>b < a</d-math> units, then our regret is
              <d-math>a -
              b</d-math>. If we accumulate the regret over <d-math>n</d-math> iterations, we get what is called <strong> cumulative regret. </strong> <br />

          GP-UCB's<d-cite key="gpucb"></d-cite> formulation is given by:
        </p>

        <d-math block>
          \alpha_{GP-UCB}(x) = \mu_t(x) + \sqrt{\beta_t}\sigma_t(x)
        </d-math>
        <p>
          Where <d-math>t</d-math> is the timestep.
        </p>

        <p>
          Srinivas et. al.<d-cite key="gpucbBounds"></d-cite> developed a schedule for <d-math>\beta</d-math> that they theoretically demonstrate to minimize cumulative regret.
        </p>

      </div>
      
      <h3>Comparison</h3>

      <p>
        We now compare the performance of different acquisition functions on the gold mining problem<d-footnote>To know more about the difference between acquisition functions look at <a href="https://www.cs.ubc.ca/~nando/540-2013/lectures/l7.pdf">these</a> amazing
          slides from Nando De Freitas</d-footnote>. We have used the optimum hyperparameters for each acquisition function.

          We ran the random acquisition function several times with different seeds. We have plotted the mean and the standard deviation (only for random acquisition (shown in shaded blue region)) below. We can see the standard deviation (variance) of the random acquisition is high, which is expected as we are randomly
          choosing points <d-math>x</d-math> to evaluate our black-box function <d-math>f</d-math>.
      </p>

      <figure>
        <d-figure><img src="images/MAB_gifs/comp.svg" /></d-figure>
      </figure>

      <p>
        We see that the on average, the <em>random</em> acquisition strategy struggles to find a good solution, whereas the other methods are more efficient in the number of iterations to find a good solution. Most acquisition functions reach fairly close to the global maxima in as few as three iterations.
      </p>


      <!--
    <h2 id="higherdimensions">Higher Dimensions</h2>

    <p>For now we have been looking at real-valued single dimensional function, i.e. <d-math>f: \mathbb{R} \texttt{ -> } \mathbb{R}</d-math> data where we needed to find the value of <d-math>\boldsymbol{x}</d-math> where we reached
      global
      maximum. Let's move on and try to tackle real-valued functions of <d-math>n</d-math> real variables functions, i.e. <d-math>f: \mathbb{R}^n \texttt{ -> } \mathbb{R}</d-math>. We will soon see that our methods that we saw earlier
      for
      the
      single dimensional case can be easily ported to multi-variable functions.</p>
    -->
      <h3 id="whyisthiseasier">Why is it beneficial to optimize the acquisition function?</h3>

      <p>
        We have replaced the original optimization problem (optimizing the given black-box function) to another optimization problem (optimization of acquisition function). How is this any better than the last problem? Evaluating the acquisition function is much cheaper. In contrast, the original black-box function is extremely costly to evaluate, by definition.
      </p>


      <h2>Hyperparameter Tuning</h2>

      <p>Before we talk about Bayesian optimization for hyperparameter tuning<d-cite key="Snoek2012,NIPS2011_4443,Bergstra"></d-cite>, we will quickly differentiate between hyperparameters and parameters: hyperparameters are set before learning and the parameters are learnt from the data. To illustrate the difference, we take the example of Ridge regression.
      </p>
      <d-math block>
        \hat{\theta}^{ridge} = argmin_{\theta\ \in \ \mathbb{R}^p} \sum\limits_{i=1}^{n} \left(y_i - x_i^T\theta \right)^2 + \lambda \sum\limits_{i=1}^{p} \theta^2_j
      </d-math>
      <p>
        In Ridge regression, the weight matrix <d-math>\theta</d-math> is the parameter, and the regularization coefficient <d-math>\lambda \geq 0</d-math> is the hyperparameter. <br />
        If we solve the above regression problem via gradient descent optimization, we further introduce another optimization parameter, the learning rate <d-math>\alpha</d-math>.
      </p>

      <p>The most common use case of Bayesian Optimization is <em>hyperparameter tuning</em>: finding the best performing hyperparameters on machine learning models.</p>

      <p>When training a model is not expensive and time-consuming, we can do a grid search to find the optimum hyperparameters. However, grid search is not feasible if functional value is extremely costly, as in the case of a large neural network that takes days to train. This might result in days of waiting to get the accuracy scores. Further, grid search scales poorly in terms of the number of hyperparameters.

      <p>We turn to BO to counter the expensiveness of getting the functional values (accuracy values) and these increased dimensions.</p>

      <h3 id="example1">Example 1 -- Support Vector Machine</h3>

      <p>Let us use an SVM on sklearn's moons dataset and try to find the optimal hyperparameter using BO. Let us have a look at the dataset first.</p>

      <figure class="smaller-img">
        <d-figure><img src="images/MAB_gifs/moons.svg" /></d-figure>
      </figure>

      <p>
        We solve the classification problem using Support Vector Machine (SVM). SVMs have two important hyperparameters,
      </p>
      <p>
        <ul>
          <li>
            <d-math>\gamma</d-math> -- modifies the behavior of the SVM's kernel. Intuitively it is a measure of the influence of a single training example<d-footnote>StackOverflow <a
                href="https://stackoverflow.com/questions/35848210/support-vector-machine-what-are-c-gamma">answer</a> for intuition behind the hyperparameters.</d-footnote>.
          </li>
          <li>
            <d-math>C</d-math> -- modifies the slackness of the classification, the higher the <d-math>C</d-math> is, the more sensitive is SVM towards the noise.
          </li>
        </ul>
      </p>
      <p>
        Let us apply Bayesian Optimization to learn the best hyperparameters for this classification task<d-footnote> <strong>Note</strong>: the surface plots you see for the Ground Truth Accuracies below were calculated for each possible hyperparameter for showcasing purposes only. We do not have these values in real applications.
        </d-footnote>. The optimum values for &#60<d-math>C, \ \gamma</d-math>&#62 have been found via running grid search at high granularity.
      </p>

      <figure class="gif-slider">
        <d-figure><img src="images/MAB_pngs/PI3d/0.05/0.png" /></d-figure>
      </figure>

      <p>Above we see a slider showing the work of the <em>Probability of Improvement</em> acquisition function in finding the best hyperparameters.</p>

      <figure class="gif-slider">
        <d-figure><img src="images/MAB_pngs/EI3d/0.01/0.png" /></d-figure>
      </figure>

      <p>Above we see a slider showing the work of the <em>Expected Improvement</em> acquisition function in finding the best hyperparameters.</p>
<!--
      <figure>
        <d-figure><img src="images/MAB_gifs/gp3d-1-2-mat.gif" /></d-figure>
      </figure>

      <p>
        Above we see a gif showing the work of the <em>Gaussian Processes Upper Confidence Bound</em> acquisition function in finding the best hyperparameters. This by far seems to perform the best with getting quite close to the global optimum value of hyperparameters (found using brute force).
      </p>

      <figure>
        <d-figure><img src="images/MAB_gifs/rand3d.gif" /></d-figure>
      </figure>

      <p>Now our favorite, the Random acquisition function.</p> -->

      <h3 id="comparison">Comparison</h3>

      <p>
        Below we will compare the acquisition functions in finding the best hyperparameters for our SVM model. We had again run the Random acquisition function several times with different seeds.
      </p>

      <figure>
        <d-figure><img src="images/MAB_gifs/comp3d.svg" /></d-figure>
      </figure>

    <p>
        We see all our acquisition functions other than the random were able to reach the best possible solution. We see the random method seemed to perform much better initially, but it could not beat the BO framework at the end of the optimization. The initial subpar performance of BO is attributed to the initial exploration.
    </p>

 

      <h3 class="collapsible">Other Examples</h3>
      <div class="content">
        <h3>Example 2 -- Random Forest</h3>

        <p>
          Using Bayesian Optimization in a Random Forest Classifier.<d-cite key="scikit"></d-cite>
        </p>

        <p>
          We will continue now to train a Random Forest on the moons dataset we had used previously to learn the Support Vector Machine model. The primary hyperparameters of Random Forests we would like to optimize our accuracy are the <strong> number</strong> of
          Decision Trees we would like to have, the <strong>maximum depth</strong> for each of those decision trees.
        </p>
        <p>
          The parameters of the Random Forest are the individual trained Decision Trees models.
        </p>
        <p>
          We will be again using Gaussian Processes with Matern kernel to estimate and predict the accuracy function over the two hyperparameters.
        </p>

        <figure class="gif-slider">
          <d-figure><img src="images/MAB_pngs/RFPI3d/0.05/0.png"></d-figure>
        </figure>

        <p>
          Above we see a gif showing the work of the <em>Probability of Improvement</em> acquisition function in finding the best hyperparameters.
        </p>

        <figure class="gif-slider">
          <d-figure><img src="images/MAB_pngs/RFEI3d/0.01/0.png"></d-figure>
        </figure>

        <p>Above we see a gif showing the work of the <em>Expected Improvement</em> acquisition function in finding the best hyperparameters.</p>

        <figure class="gif-slider">
          <d-figure><img src="images/MAB_pngs/RFGP_UCB3d/1-2/0.png"></d-figure>
        </figure>

        <p>
          Above we see a gif showing the work of the <em>Gaussian Processes Upper Confidence Bound</em> acquisition function in finding the best hyperparameters.
        </p>

        <figure class="gif-slider">
          <d-figure><img src="images/MAB_pngs/RFRand3d/1-2/0.png"></d-figure>
        </figure>

        <p>Let us now use the Random acquisition function.</p>

        <figure>
          <d-figure><img src="images/MAB_gifs/RFcomp3d.svg"></d-figure>
        </figure>

        <p>
          Looking at the ground truth, we see the black-box function we are trying to optimize is not too smooth, and therefore we see that our optimization strategies seem to struggle compared to the last example. This shows that the effectiveness of BO depends on the surrogate's efficiency to model the actual black-box function. It is still interesting to notice that the BO framework beats the random strategy.
        </p>

        <h3>Example 3 -- Neural Networks</h3>
        <p>
          Let us take this example to get an idea of how to apply Bayesian Optimization to training neural networks like CNNs on Mnist. Here we will be using <d-code language="python">scikit-optim</d-code>, which also provides us support for
          optimizing
          our function on a mix of categorical, integral, and real variables. We will not be plotting the ground truth here, as it is extremely costly to do so. Below are some code snippets that go into adding Bayesian Optimization for hyperparameter
          tuning.
        </p>

        <p>
          The code below declares the search space for the optimization problem (hyperparameter tuning). In this example we are limiting the search space to be the following:
          <ul>
            <li>
              batch_size -- Our search space for the possible batch sizes consists of integer values s.t. batch_size = <d-math>2^i \ \forall \ 2 \leq i \leq 7 \ \& \ i \in \mathbb{Z}</d-math>.<br />
              This hyperparameter sets the number of training examples to combine to find the gradients for a single step in gradient descent.
            </li>
            <li>
              learning rate -- We will be searching all the real numbers from the range <d-math>[10^{-6}, \ 1]</d-math>. We will be using logarithmic uniform distribution as our prior distribution if we are to sample random points from this space.<br />
              This hyperparamter sets the stepsize with which we will perform gradient descent in the neural network.
            </li>
            <li>
              activation -- We will have one categorical variable, i.e. the activation to apply to our neural network layers. This variable can take on values in the set <d-math>\{ relu, \ sigmoid \}</d-math>.
            </li>
          </ul>
        </p>

        <d-code block language="python">
          log_batch_size = Integer(
          low=2,
          high=7,
          name='log_batch_size'
          )
          lr = Real(
          low=1e-6,
          high=1e0,
          prior='log-uniform',
          name='lr'
          )
          activation = Categorical(
          categories=['relu', 'sigmoid'],
          name='activation'
          )

          dimensions = [
          dim_num_batch_size_to_base,
          dim_learning_rate,
          dim_activation
          ]
        </d-code>

        <p>
          Moving on, we have a minimizer function imported from <d-code language="python">scikit-optim</d-code> called <d-code language="python">gp-minimize</d-code>. One can change the acquisition function from a number of available options.
          Below we have a code snippet showing the calling of the optimization function with the <em>Expected Improvement</em> acquisition function.
        </p>
        <p>
          <strong>Note</strong>: One will need to negate the accuracy values as we are using the minimizer function from <d-code language="python">scikit-optim</d-code>.
        </p>

        <d-code block language="python">
          # setting up default parameters (1st point)
          default_parameters = [4, 1e-1, 'relu']

          # bayesian optimization
          search_result = gp_minimize(
          func=train,
          dimensions=dimensions,
          acq_func='EI', # Expected Improvement.
          n_calls=11,
          x0=default_parameters
          )
        </d-code>

        <figure class="smaller-img">
          <d-figure><img src="images/MAB_gifs/conv.svg"></d-figure>
        </figure>

        <p>
          We now have the hyperparameters that have maximized your accuracy. In the graph above the y-axis denotes the <d-math>\left( f(x^+) \right)</d-math> and the x-axis denotes the number of times we have queried <d-math>(t)
          </d-math> the neural network with some set of given hyperparameters.
        </p>

        <p>
          We can apply the BO for more dimensions (more hyperparameters), even if the dimensions which are categorical (as 'relu' vs 'sigmoid' above) can be incorporated into BO as it is done in scikit-optim.
        </p>

        <p>
          Looking at the above example, we can see that incorporating Bayesian Optimization isn't a big problem and saves a lot of time we can see that the network was able to get to an accuracy of nearly one in around three iterations. That is
          impressive! The example above has been inspired by Hvass Laboratories' Tutorial<d-footnote> <a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/19_Hyper-Parameters.ipynb">Notebook</a> showcasing hyperparameter
            optimization in Tensorflow. </d-footnote> on <d-code language="python">scikit-optim</d-code>.
        </p>

        <p>
          While running the experiment on our laptops, each of the evaluation cost us an approximate of 15 minutes. Looking at the number of evaluations required by the Bayesian Optimization approach, we were able to get to an accuracy of nearly 1.0 in just 5 iterations.
        </p>
        <p>
          The parameters selected by the optimizer were <d-math langugage="python">[4, 0.0019, 'relu']</d-math>, i.e., batch size of 8, learning rate of 1.9e-2 and 'relu' activation function.
        </p>
        <p>
          If we had performed a naive grid search, it would have taken us a lot more iterations <d-math>(5 \times 2 \times 7)</d-math>, and we still would not have tested a learning rate near the learning rate returned by the optimizer. Suppose using grid search resulted in us taking twenty-five iterations. If we convert it to the time we spent over the Bayesian Optimization approach, we will get a better idea of the amount of time that can be saved when evaluation of the ground truth functions is even more costly.
        </p>

        <p>
          We would have saved around five hours by using BO for our hypothetical scenario of using Grid Search.
        </p>
      </div>

        <h1 id="conclusions">Conclusion and Summary</h1>

        <p>
          In this article, we looked at Bayesian Optimization, which involves optimizing a black-box function. Our primary focus is on the cases when the function evaluations are expensive, making grid or exhaustive search impractical. We looked at the key components of BO. First, we looked at the notion of using a surrogate function (with a prior over the space of objective functions) to model our black-box function. Next, we looked at the "Bayes" in BO - the function evaluations are used as data to obtain the surrogate posterior. We look at acquisition functions, which are functions of the surrogate posterior and are optimized sequentially. This new sequential optimization is in-expensive and thus of utility of us. We also looked at a few acquisition functions and showed how these different functions balance exploration and exploitation. Finally, we looked at some practical examples of BO for optimizing hyper-parameters for machine learning models. We believe we have presented an interesting and effective suite of black-box optimization techniques. We now hope that we have whetted the reader's appetite!
        </p>

    <h2 id="embracebayesianoptimization">Embrace Bayesian Optimization</h2>

    <p>
      Optimizing or tuning hyperparams is an important facet of modern machine learning algorithms and BO is an efficient way of tuning the same.
    </p>
    <p>
      Having read all the way through, you might have been sold on the idea about the time you can save by asking Bayesian Optimizer to find the best hyperparameters for your fantastic model. There are a plethora of Bayesian Optimization libraries available. We have linked a few below. Do check them out.
    </p>

    <ul>
      <li><a href="https://scikit-optimize.github.io/">scikit-optimize</a>
        <d-footnote>Really nice tutorial showcasing hyperparameter optimization on a neural network available at this <a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/19_Hyper-Parameters.ipynb">link</a>.
        </d-footnote>
      </li>

      <li><a href="https://app.sigopt.com/docs/overview/python">sigopt</a></li>

      <li><a href="http://hyperopt.github.io/hyperopt/">hyperopt</a></li>

      <li><a href="https://github.com/HIPS/Spearmint">spearmint</a></li>

      <li><a href="https://github.com/Yelp/MOE">MOE</a></li>
    </ul>

    <p>
      We hope you had a good time reading the article and hope you are ready to <strong>exploit</strong> the power of BO. In case you wish to <strong>explore</strong> more, please read the <a href="#FurtherReading">Further Reading</a> section
      below.
    </p>




    <h2 id="ack">Acknowledgements</h2>

    <p>
      This article was made possible with inputs from numerous people. Firstly, we would like to thank all the Distill reviewers for their punctilious and actionable feedback. These fantastic reviews immensely helped strengthen our article. We further express our gratitude towards the Distill Editors, who were extremely kind and helped us navigate various steps to publish our work. We would also like to thank <a href="https://sgarg87.github.io/">Dr. Sahil Garg</a> for his feedback on the flow of the article. Lastly, we would like to acknowledge the help we received from <a href="https://colah.github.io/">Christopher Olah</a> and the <a href="http://initiatives.iitgn.ac.in/writingstudio/wp/">Writing Studio</a> to improve the script of our article.
    </p>

  </d-article>

  <d-appendix>

    <h3 id="FurtherReading">Further Reading</h3>
    <ol>
      <li>
        <p>Using gradient information when it is available.</p>
        <ul>
          <li>
            Suppose we have gradient information available, we should possibly try to use the information. This could result in a much faster approach to the global maxima. To know more about this exciting domain of research look at the paper by
            Wu, et. al.<d-cite key="BOwtGD"></d-cite>.
          </li>
        </ul>
      </li>
      <li>
        <p>
          To have a quick view of differences between BO and Gradient Descent, one can look at <a href="https://stats.stackexchange.com/q/161936">this</a> amazing answer at StackOverflow.
        </p>
      </li>
      <li>
        <p>
          We talked about optimizing a black-box function here. If we are to perform over multiple objectives, how do these acquisition functions scale? There has been fantastic work in this domain too! We try to deal with these cases by having multiobjective acquisition functions. Have a look at <a href="https://gpflowopt.readthedocs.io/en/latest/notebooks/multiobjective.html">this excellent</a> notebook for an example using <d-code language="python">gpflowopt</d-code>.
        </p>
      </li>
      <li>
        <p>
          One of the more interesting uses of hyperparameters optimization can be attributed to searching the space of neural network architecture for finding the architectures that give us maximal predictive performance. One might also want to consider multiobjective optimizations as some of the other objectives like memory consumption, model size, or inference time also matter in practical scenarios.
        </p>
      </li>
      <li>
        <p>
          When the datasets are extremely large, human experts tend to test hyperparameters on smaller subsets of the dataset and iteratively improve the accuracy for their models. There has been work in Bayesian Optimization, taking into account these approaches<d-cite key="hyperband,largeBO"></d-cite> when datasets are of such sizes.
        </p>
      </li>
      <li>
        <p>
          There also has been work on BO, where one explores with a certain level of "safety", meaning the evaluated values should lie above a certain security threshold functional value<d-cite key="SafeExplore"></d-cite>. One toy example is the possible configurations for a flying robot to maximize its stability. If we tried a point with terrible stability, we might crash the robot, and therefore we would like to explore the configuration space more diligently.
        </p>
      </li>
      <li>
        <p>
          We have been using GP in our BO for getting predictions, but we can have any other predictor or mean and variance in our BO.
        </p>
        <ul>
          <li>
            <p>
              One can look at <a href="http://aad.informatik.uni-freiburg.de/~hutter/ML3.pdf">this</a> slide deck by Frank Hutter discussing some limitations of a GP-based Bayesian Optimization over a Random Forest based Bayesian Optimization.
            </p>
          </li>
          <li>
            <p>
              There has been work on even using deep neural networks in BO<d-cite key="NNbasedBO"></d-cite> for a more scalable approach compared to GP. The paper talks about how GP-based Bayesian Optimization scales cubically with the number of observations, compared to their novel method that scales linearly.
            </p>
          </li>
        </ul>
      </li>
      <li>
        <p>
          Things to take care when using BO.
        </p>
        <ul>
          <li>
            <p>
              While working on the blog, we once scaled the accuracy from the range <d-math>[0, \ 1]</d-math> to <d-math>[0, \ 100]</d-math>. This changed broke havoc as the Gaussian Processes we were using had certain hyperparameters, which needed
              to be scaled with the accuracy to maintain scale invariance. We wanted to point this out as it might be helpful for the readers who would like to start using on BO.
            </p>
          </li>
          <li>
            <p>
              We need to take care while using Bayesian Optimization. Bayesian Optimization based on Gaussian Processes Regression is highly sensitive to the kernel used. For example, if you are using <a
                href="https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Matern.html">Matern</a> kernel, we are implicitly assuming that the function we are trying to optimize is first order differentiable.
            </p>
          </li>
          <li>
            <p>
              Searching for the hyperparameters, and the choice of the acquisition function to use in BO are interesting problems in themselves. There has been amazing work done, looking at this problem. As mentioned previously in the post, there has
              been work done in strategies using multiple acquisition function<d-cite key="multiACQ"></d-cite> to deal with these interesting issues.
            </p>
          </li>
          <li>
            <p>
              A nice list of tips and tricks one should have a look at if you aim to use Bayesian Optimization in your workflow is from this fantastic post by Thomas on <a href="https://thuijskens.github.io/2016/12/29/bayesian-optimisation/">Bayesian
                Optimization with sklearn</a>.
            </p>
          </li>
        </ul>
      </li>
      <li>
        <p>
          BO applications.
        </p>
        <ul>
          <li>
            <p>
              BO has been applied to Optimal Sensor Set selection for predictive accuracy<d-cite key="sensorBO"></d-cite>.
            </p>
          </li>
          <li>
            <p>
              Pater Faizer in his <a href="https://www.youtube.com/watch?v=c4KKvyWW_Xk">talk</a> mentioned that Uber uses Bayesian Optimization for tuning algorithms via backtesting.
            </p>
          </li>
          <li>
            <p>Facebook<d-cite key="letham2019"></d-cite> uses Bayesian Optimization for A/B testing.
          </li>
          <li>
            <p>
              Netflix and <a href="https://engineeringblog.yelp.com/2014/10/using-moe-the-metric-optimization-engine-to-optimize-an-ab-testing-experiment-framework.html">Yelp</a> use Metrics Optimization software like <a href="http://github.com/Yelp/MOE">Metrics Optimization Engine (MOE)</a> which take advantage of Parallel Bayesian Optimization<d-cite key="yelpBO"></d-cite>.
            </p>
          </li>
        </ul>
      </li>
    </ol>

    <d-bibliography src="references.bib"></d-bibliography>
  </d-appendix>

</body>
</html>
