<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Exploring Bayesian Optimization</title>
  <script src="js/template.v2.js"></script>
  <link rel="stylesheet" type="text/css" href="css/styles.css">
  <!-- <script defer src="js/bundle.js"></script> -->
  <!-- <link rel="stylesheet" type="text/css" href="css/style.css"> -->

</head>
<body>

<d-front-matter>
  <script id='distill-front-matter' type="text/json">{
  "title": "Exploring Bayesian Optimization",
  "description": "Breaking Bayesian Optimization into small, sizable chunks.",
  "authors": [
    {
      "author":"Apoorv Agnihotri",
      "authorURL":"https://apoorvagnihotri.github.io/",
      "affiliations": [{"name": "Indian Insitute of Gandhinagar", "affiliationURL":
      "https://www.iitgn.ac.in/"}]
    },
    {
      "author":"Nipun Batra",
      "authorURL":"https://nipunbatra.github.io/",
      "affiliations": [{"name": "Indian Insitute of Gandhinagar", "affiliationURL": "https://www.iitgn.ac.in/"}]
    }
  ],
  "katex": {
    "delimiters": [
      {"left": "$$", "right": "$$", "display": false}
    ]
  }
  }</script>
</d-front-matter>

<d-title style="padding-bottom: 0">
  <h1>Exploring Bayesian Optimization</h1>
  <p>Breaking Bayesian Optimization into small, sizable chunks.</p>

  <!-- <d-figure id="Teaser" class="l-screen shaded-figure" style="border-bottom: none"></d-figure> -->

</d-title>

<d-byline></d-byline>

<d-article>
<p>In this post, we are going to focus on two tasks, prediction and optimization of a function that is extremely costly to evaluate. We query the user/oracle to label (evaluate) samples — Oracle than returns us a scalar value. We will be trying to pose the problems first and then talk about some of the ways to solve these problems.</p>

<p>The primary motivation behind both of the tasks is the expensive cost of labeling (evaluating).</p>

<h2 id="mininggold">Mining Gold!</h2>
<p>Let us explain the two problems using the gold mining application.
We will, for now, look at only one-dimensional locations, i.e., we are talking gold distribution only about a line.
The issue we have is that at the start of the activity, we have no idea about the amount of gold at different locations.
The only way we can get the information about the amount of gold is by drilling at a location.
This drilling is costly and involves expensive sensors to be used.
We, therefore, want to minimize the number of drillings that we require.</p>

<p>We below show two of the common objectives for the gold mining problem.</p>

<ul>
<li><p><strong>Problem 1: Best Estimate of Gold Distribution</strong>
In this problem, we are supposed to estimate the amount of gold on the one-dimensional line. But we can not drill at every location. We should drill at those locations that provide us the "maximum" information about the distribution of the gold.</p></li>

<li><p><strong>Problem 2: Location of Maximum Gold</strong>
In this problem, we are supposed to find the location in the one-dimensional space where the gold quantity is the maximum. This problem focuses on finding the location with the most gold content.</p></li>
</ul>

  <figure>
  <d-figure><img src="images/MAB_gifs/active-gp.gif"/></d-figure>
  </figure>

  <figure>
  <d-figure><img src="images/MAB_gifs/mab-gp-pi-eps0.5.gif"/></d-figure>
  </figure>

<ul>
<li><p>Here are representative animations showing the process of drilling at new locations and to reduce the uncertainty and get the best predictions showcasing the <strong>Active Learning</strong> problem.</p></li>

<li><p>And drilling at locations to get the location of the maximum gold reserve, showcasing the <strong>Bayesian Optimization</strong> problem</p></li>
</ul>

<p>We will build the solution to both of these problems from the ground up.</p>



<h3 id="activelearning">Active Learning</h3>

<p><strong>Problem 1</strong> is very similar to problems we like to solve using active learning. Active learning is used to predict distribution by reducing uncertainty. One of the ways we can reduce the uncertainty is by choosing the point at which we have the maximum variance (we are most uncertain).</p>

<h3 id="gaussianprocesses">Gaussian Processes</h3>

<p>
  One might want to look at this excellent distillpub article<d-cite key="görtler2019a"></d-cite> on Gaussian Processes<d-cite key="Rasmussen2004"></d-cite>.
  As you can remember, we use Gaussian Processes to get a prediction as well as the attached uncertainty (variance) with that prediction.
  This will turn out to be useful for us, as we wanted to drill where we were most uncertain.
  By using Gaussian processes, we take some naive assumption that the gold distribution of nearby points is similar (smoothness).
</p>

<p>
  Let us now try to see how our groundtruth data looks like.
</p>
  <figure class="smaller-img">
  <d-figure><img src="images/MAB_gifs/GT.svg"/></d-figure>
  </figure>


<h4 id="priormodel">Prior Model</h4>

<p>Our prior model doesn't know much and assumes a smooth relationship between points via an Matern kernel. The Grey line in the graph below denotes the knowledge we have about the gold content without drilling even at a single location.</p>

  <figure class="smaller-img">
  <d-figure><img src="images/MAB_gifs/prior.svg"/></d-figure>
  </figure>

<p>
  Also, take notice that the confidence (uncertainty) about the gold content is also the same for every location.
</p>

<h4 id="addingtrainingdata">Adding Training Data</h4>

<p>Let us now add a point to the train set or in other words, drill one of the locations and see the gold content (<d-code language="python">y</d-code>). We can see how our confidence and our estimates change after we get this first information by fitting the model to the new data. I am going to add <d-code language="python">(x = 0.5, y = f(0.5))</d-code language="python"> into the train set now.</p>

  <figure>
  <d-figure><img src="images/MAB_gifs/posterior.svg"/></d-figure>
  </figure>

<p>Nice! We see now that the posterior has changed and we are very certain about the gold content in the vicinity of <d-code language="python">x = 0.5</d-code>, but, very uncertain far away from it. Also, we can see that the mean of the point closer to <d-code language="python">x = 0.5</d-code> is closer to the value that we got from drilling and seeing the gold content. So, we now come to the key idea.</p>

<h4 id="activelearningprocedure">Active Learning Procedure</h4>

<ol>
  <li>Choose the point of having the highest uncertainty</li>
  <li>Add the point to train set</li>
  <li>Train on the new train set</li>
  <li>Go to 1 till convergence or budget elapsed</li>
</ol>


<p>
  Let us now automate this process and see how our posterior changes at every iteration where we add a sensor. For each of our iteration below, the prior was the Gaussian Process learned on the points already in the training set. We have recreated the 1st animation at the top of the post!
</p>
  <figure>
  <d-figure><img src="images/MAB_gifs/active-gp.gif"/></d-figure>
  </figure>

<p>
  There you go we have recreated one of the plots from the starting of the blog! One point to notice is that this idea of choosing the most uncertain location leads to querying of the points that are the farthest (visible when we choose the 2nd location to drill). This might not be so good as we are kind of wasting our drillings because they are at the boundary of the 1-dimensional plot.
</p>

<h3 id="bayesianoptimization">Bayesian Optimization</h3>


<p><strong>Problem 2</strong> requires us to find the location where the gold content is maximum. Even though the problem setting may be similar, the objective is quite different than problem 1. In other words, we just want the location where we can drill to get the most gold.</p>

<p>Older problem - Earlier in the active learning problem, our motivation for drilling at locations was to predict the distribution of the gold content over all the locations in the one-dimensional line. We, therefore, had chosen the next location to drill where we had maximum uncertainty about our estimate.</p>

<p>In this problem, we are instead interested to know the location at which we find the maximum gold. For getting the location of maximum gold content, we might want to drill at the location where predicted mean is the highest (exploit). But unfortunately our mean is not always accurate, so we need to correct our mean (reduce variance / explore) too. Bayesian Optimization looks at both exploitation and exploration, whereas in the case of Active Learning Problem, we only cared about exploration.</p>


<h4 id="acquisitionfunctions">Acquisition Functions</h4>

<p>
  Now, to take into account the combination of exploration and exploitation, we try to use a function which combines the two sides. These utility functions that take into account both exploration and exploitation in multi-arm bandit problem are called acquisition functions.
</p>
  <figure>
  <d-figure><img src="images/MAB_gifs/acq_fn.svg"/></d-figure>
  </figure>

<p>
  Here, we can see that mean near the location of the just added point (red point) is high. But as we go far from the red point, we see that our uncertainty increases to a maximum. As we discussed in multi-arm bandit problem, we like to have some combination of exploration and exploitation. The most basic way to do so is by linearly combining the two values.
</p>

<hr />

<h3 id="acq">ACQ1</h4>

<d-math block> <!-- class="l-middle.side" -->
  \texttt{acq\_fn}(x) = \mu + \lambda \times \sigma
</d-math>

<p>
  This combined value that takes into account exploration and exploitation is referred to as the acquisition value, returned by acquisition function. We see at around the location <d-code language="python">x = 1.4</d-code> we get the maximum value for the acquisition (green curve). Thus we next select this location to drill.
</p>

<p>
  The intuition of using the acquisition function <d-code language="python">mean + lam * uncertainty</d-code> is that we are interested in finding the global mean, so taking into account the estimated mean would be a good idea. Additionally, we would like to explore too (using <d-code language="python">lam</d-code>); else we might be stuck in a local minimum if don't explore too much (see below).
</p>

<p>
  Let us now try different hyperparameters for <d-code language="python">ACQ1</d-code>. We can see that on increasing <d-code language="python">lam</d-code> we "explore" more! In the below case we can easily see since we didn't give too much importance to the uncertainty (low <d-code language="python">lam</d-code>), we got stuck in local minima.
</p>
  <figure>
  <d-figure><img src="images/MAB_gifs/mab-gp-1.gif"/></d-figure>
  </figure>

<p>
  Below we can see that this choice of <code>lam</code> is still a little smaller than we like (we would like to see that we get to exploit the location where the gold is the most.).
</p>
  <figure>
  <d-figure><img src="images/MAB_gifs/mab-gp-5.gif"/></d-figure>
  </figure>

<p>
  Perfect! We see that setting this value of <code>lam = 10</code> resulted in finding points near the global maxima and not getting stuck in a local maximum.
</p>
  <figure>
  <d-figure><img src="images/MAB_gifs/mab-gp-10.gif"/></d-figure>
  </figure>

<hr />

<h3 id="random">Random</h3>

<p>We had used a little intelligent acquisition function earlier, let's see is out acquisition function is not that intelligent and chooses randomly.</p>

<p>We have here implemented a random method as a baseline. Notice, random method can find a location near the global maximum but is not able to exploit (try to find the global maxima that might be near this "best" location). Instead, it randomly chooses to explore (not even intelligently) here and there. Even with no intelligent, we might get good locations which might be close to the location with the most gold content.
</p>
  <figure>
  <d-figure><img src="images/MAB_gifs/rand.gif"/></d-figure>
  </figure>

<hr />

<h3 id="probabilityofimprovementpi">Probability of Improvement (PI)</h3>

<p>
  Let us look into our next method for the MAB maximization problem. As before, we want to balance or trade-off between exploration and exploitation. The idea behind the algorithm is fairly simple - choose the next point as the one which has the highest probability of improvement over the current max <d-math>(\mu^+)</d-math>.
</p>

<p>Let's understand this concept via two cases:</p>

<ol>
  <li><p>
    We have two points of similar means (of function values (gold in our case)). We now want to choose one of these to obtain the labels or values. We will choose the one with higher variance. This basically says that given same exploitability, we choose the one with higher exploration value.
  </p></li>
  <li><p>
    We have two points having same variance. We would now choose the point with the higher mean. This basically says that given same explorability, we will choose the one with higher exploitation value.
  </p></li>
</ol>
<ol>
  <li><p>
    Let <d-math>\mu^+</d-math> be the current highest value of the function
  </p></li>
  <li><p>
    Let <d-math>\epsilon</d-math> be close to zero
  </p></li>

  <li><p>
    Choose <d-math>x^* = argmax(P(f(x)) > (\mu^+ +\epsilon))</d-math>
  </p></li>
</ol>

<p>This can be given as: <d-math>x^* = argmax_x \Phi(\frac{\mu(x) - \mu^+ - \epsilon}{\sigma(x)})</d-math> where
<d-math>\Phi(\cdot)</d-math> indicates the CDF.</p>


<h4 id="intuitionbehindtheformula">Intuition behind PI</h4>

<p>Below is a graph that helps to visualize how the PI values are calculated. We have calculated for 3 points <d-code language="python">x in [0.10, 0.6, 4]</d-code language="python">. We can see the CDF being shaded in the graphs below. Further, we can see if we increase <d-code language="python">eps</d-code>, we implicitly place more importance to the uncertainty of a point. If <d-code language="python">eps</d-code> is increased, the points with a larger sigma will benefit as their probability density is spread more. Thus points with more spread out sigma would have a higher value of cumulative density function on same <d-math>\mu^+ + \epsilon</d-math>.</p>

  <figure>
  <d-figure><img src="images/MAB_gifs/pi_cdf.gif"/></d-figure>
  </figure>

<p>
  <d-math>\mu^+</d-math> refers to the maximum functional value i.e., <d-code language="python">max(train_y)</d-code>, where <d-code language="python">train_y</d-code> refers to the the gold content at the currently drilled locations.
  We see that the probability of improvement values are calculated by finding the functional value of the cumulative density function at <d-math>\mu^+</d-math>. The Gaussian parameters for each point are the mean and standard deviation predicted from Gaussian Process Regressor for that point.
</p>

<h4 id="eps">Hyperparameter: Eps</h4>
<p>
  Now we possess the intuition behind how Probability of Improvement is calculated, now let's change <d-code language="python">eps</d-code> and look at its effects.
</p>

  <figure>
  <d-figure><img src="images/MAB_gifs/mab-gp-pi-eps0.01.gif"/></d-figure>
  </figure>
<p>
  Looking at the graph above we can see that we are not effectively exploring at value <d-code language="python">eps = 0.01</d-code> for the Probability of Improvement acquisition function. We are stuck.
</p>

  <figure>
  <d-figure><img src="images/MAB_gifs/mab-gp-pi-eps0.5.gif"/></d-figure>
  </figure>
<p>
  Looking above, we see increasing the value to <d-code language="python">eps = 0.5</d-code>, enables us to explore more and get to the maximum value which we wanted in the case of Multi-arm bandit problem. One can notice that the predicted values where <d-math>x \in [3, 4.5]</d-math> posses uncertainty (can be identified by the grey translucent area, but as we remember we are not interested in getting the best prediction of the gold distribution, we only care about the maximum value that we can achieve, which this acquisition function with given hyper-parameters is able to capture nicely!
</p>

<p>
  Let's look at what happens if we increase the hyper-parameter <d-code language="python">eps</d-code> a bit more.
</p>

  <figure>
  <d-figure><img src="images/MAB_gifs/mab-gp-pi-eps3.gif"/></d-figure>
  </figure>
<p>
  We see that we made things worse! Our model now uses <d-code language="python">eps = 3</d-code> which has effectively resulted in way too much exploration. This amount of exploration is not able to exploit when we land somewhere near a global maximum.
</p>


<hr />


<h3 id="expectedimprovementei">Expected Improvement (EI)</h3>

<p>
  Probability of improvement only looked at <d-code language="python">how likely</d-code> is an improvement, but, shouldn't we be looking into <d-code language="python">how much</d-code> we can improve. The next criterion called, Expected Improvement, (EI) looks into both :)
</p>

<d-math block>
EI(x)=
\begin{cases}
  (\mu(x) - \mu^+ - \epsilon)\Phi(Z) + \sigma(x)\phi(Z), & \text{if}\ \sigma(x) > 0 \\
  0 & \text{if}\ \sigma(x) = 0 
\end{cases}

Z= \frac{\mu(x) - \mu^+ - \epsilon}{\sigma(x)}
</d-math>
<p>
  where <d-math>\Phi(\cdot)</d-math> indicates CDF and <d-math>\phi(\cdot)</d-math> indicates pdf.
</p>
<p>We can see when our <em>Expected Improvement</em> will be high.</p>

<ul>
<li>It is high when the expected value of mean(x) - $\mu^+$ is high.</li>

<li>It is high when the uncertainty around a point is high.</li>
</ul>

<p>Now, if we see the role of $\epsilon$ in <em>Expected Improvement</em>, it is the exact same as the role played in the case of <em>Probability of Improvement</em> (we have the same expression in PI) <d-footnote>A good introduction to the Expected Improvement Acuqisition function is by <a href="https://thuijskens.github.io/2016/12/29/bayesian-optimisation/"
 target="_blank">this post</a> by Thomas Huijskens and <a href="https://people.orie.cornell.edu/pfrazier/Presentations/2018.11.INFORMS.tutorial.pdf" target="_blank">these slides</a> by Peter Frazier</d-footnote>.</p>

  <figure>
  <d-figure><img src="images/MAB_gifs/mab-gp-ei-eps0.01.gif"/></d-figure>
  </figure>

<p>
  Like the Probability of Improvement's acquisition function, we can moderate the amount of explorability the Expected Improvement's acquisition function by setting the <d-code language="python">eps</d-code> hyper-parameter.
</p>

<p>
  We see that having <d-code language="python">eps = 0.01</d-code> primarily results in exploitation, and we are not able to get to the global maxima due to this myopic drilling location selection.
</p>

<p>
  Let's try increasing the <d-code language="python">eps</d-code> variable to focus a little more on exploribility.
</p>

  <figure>
  <d-figure><img src="images/MAB_gifs/mab-gp-ei-eps1.5.gif"/></d-figure>
  </figure>
<p>
  As we expected, increasing the value to <d-code language="python">eps = 1.5</d-code> makes the acquisition function explore more and exploit when the time comes. We see that it moves slowly once it reaches near the global maxima, trying to find the global maxima. In this case, the exploration is effectively helping us reach a higher functional value much earlier!
</p>

<p>
  Let's see if increasing <d-code language="python">eps</d-code> helps us more!
</p>

  <figure>
  <d-figure><img src="images/MAB_gifs/mab-gp-ei-eps3.gif"/></d-figure>
  </figure>
<p>
  Is this better than before? Turns out a yes and a no. We see that here we do too much exploration given the value of <d-code language="python">eps = 3</d-code>. Which results in early reaching something close to global maxima, but unfortunately we don't exploit to get more gains near the global maxima. We would have liked an acquisition function that tried to exploit a bit more after reaching somewhere close to the global maxima. In essence:
<ul>
  <li>reach near global maxima in a lower number of iterations</li>
  <li>we don't exploit once we reach near global maxima</li>
</ul>
</p>

<hr />

<p>
  We have seen two closely related methods, The <em>Probability of Improvement</em> and the <em>Expected Improvement</em>. We can think of these two to be related to the ideas that we commonly are familiar with <d-code language="python">risk</d-code> and <d-code language="python">gain</d-code> respectively.
</p>

<p>
  It seems natural to see how these metrics change for each of the points. We have plotted the values for both policies' acquisition function's values below, for each of the possible locations. The graph shows the relation followed between EI and PI for when we have a single training point <d-code language="python">(0.5 f(0.5))</d-code>.
</p>

  <figure class="smaller-img">
  <d-figure><img src="images/MAB_gifs/Ei_Pi_graph/0.svg"/></d-figure>
  </figure>
<p>
  If we look closely, we can see if we have an equal estimated improvement as in the case with the points with <d-code language="python">EI(x) = 0.4</d-code> it would be more beneficial to differentiate between these points which have a better value for Probability of Improvement. In other words, when <d-code language="python">gain</d-code>s are the same, we should prioritize to choose the option with lesser <d-code language="python">risk</d-code>. And similarly, when the <d-code language="python">risk</d-code>s are similar, we would likely want to go with points with greater <d-code language="python">gain</d-code>s.
</p>

<hr />

<h3 id="gaussianprocessupperconfidenceboundgp_ucb">Gaussian Process Upper Confidence Bound (GP_UCB)</h3>

<p>
  GP_UCB is another formulation for acquisition function where we also have theoretical bounds on the number of iterations taken to reach near global maximum.
</p>

  <figure>
  <d-figure><img src="images/MAB_gifs/mab-gp-gp_ucb1-1.gif"/></d-figure>
  </figure>
<p>
  We seem to be exploiting too much, let's increase the exploratory hyperparameters!
</p>

  <figure>
  <d-figure><img src="images/MAB_gifs/mab-gp-gp_ucb3-1.gif"/></d-figure>
  </figure>
<p>
  Using this set of hyperparameters, we are able to get near global maxima and further "exploit" to find the global maximum. This was a result of increasing the value of <d-code language="python">v</d-code> to <d-code language="python">3</d-code>; this shows that <d-code language="python">v</d-code> gives weight to exploration.</p>

  <figure>
  <d-figure><img src="images/MAB_gifs/mab-gp-gp_ucb1-3.gif"/></d-figure>
  </figure>
<p>
  Setting the values of the hyperparameters  to <d-code language="python">v = 1</d-code> and <d-code language="python">delta = 3</d-code> results a greater exploitation.</p>

<hr />

<h3 id="probabilityofimprovementdlambdatimesdexpectedimprovementei_pi">Probability of Improvement + <d-math>\lambda \  \times</d-math> Expected Improvement (EI_PI)</h3>

<p>Below we have tried to combine PI and EI using a linear combination as a combination of various acquisition function also results in an acquisition function. We can, therefore, combine any of the acquisition function and form a new one.</p>

  <figure>
  <d-figure><img src="images/MAB_gifs/mab-gp-ei_pi.gif"/></d-figure>
  </figure>
<hr />

<h3 id="comparisonbetweenallthemethods">Comparison between all the methods</h3>

<p>Below we have a graph showing a comparison between the methods discussed above. We have chosen the hyper-parameters that gave us the best performance during our basic hyper-parameter search.</p>

  <figure>
  <d-figure><img src="images/MAB_gifs/comp.svg"/></d-figure>
  </figure>
<p>We see the <em>Random</em> method is able to find the maximum much before any of the other methods, this can be seen when we are faced with smaller spaces to find the global maximum. If we have more dimensions to <d-code language="python">x</d-code>, searching in this space would not be so easy using random, due to what we call the curse of dimensionality.</p>

<hr />

<h2 id="observation">Observation</h2>

<p>We see that the plots above show the maximum gold content detected for the case of multi-arm bandit problem vs. the number of holes drilled. Looking at the graph above we can see that for our problem Probability of Improvement performed the best among all the variants of Acquisition functions.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We see that for the case of multi-arm bandit we have a bit different problem as compared to the active learning problem and therefore we have the different objective functions that we try to maximize for the query points.</p>

<h3 id="caution">Caution</h3>

<p>We need to take care while using Bayesian Optimization</p>

<ul>
<li>The scale of the features</li>

<li>Kernel selection for Gaussian Process</li>
</ul>






<d-appendix>
    <h3 id="AuthorContributions">Author Contributions</h3>
    <p>
      We thank Apoorv for working with the formation of the post and experimentation of different Bayesian Optimiazation methods.<br/>
      We would like to thank Nipun Batra for guiding us throughout the formation of the post.
    
    <h3 id="FurtherReading">Further Reading</h3>
    <p>The following papers offer a more in-depth exploration of Bayesian Optimization methods.</p>
    <ul>
      <li><a href="http://www.tmpl.fi/gp/">Gaussian process regression demo</a> by Tomi Peltola</li>
      <li><a href="http://katbailey.github.io/post/gaussian-processes-for-dummies/">Gaussian Processes for Dummies</a> by Katherine Bailey</li>
      <li><a href="https://blog.sigopt.com/posts/intuition-behind-gaussian-processes">Intuition behind Gaussian Processes</a> by Mike McCourt</li>
      <li><a href="https://blog.dominodatalab.com/fitting-gaussian-process-models-python/">Fitting Gaussian Process Models in Python</a> by Chris Fonnesbeck</li>
    </ul>
  <p>If you want more of a hands-on experience, there are also many Python notebooks available:</p>
  <ul>
    <li><a href="https://blog.dominodatalab.com/fitting-gaussian-process-models-python/">Fitting Gaussian Process Models
      in Python</a> by Chris Fonnesbeck
    </li>
    <li><a href="http://nbviewer.jupyter.org/github/adamian/adamian.github.io/blob/master/talks/Brown2016.ipynb/">Gaussian
      process lecture</a> by Andreas Damianou
    </li>
  </ul>

  <d-bibliography src="references.bib"></d-bibliography>
</d-appendix>

</body>
</html>
